{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><span style=\"font-size: 36pt; font-family: georgia, palatino, serif; color: #800000;\">Learning Topical Social Sensors</span></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><span style=\"color: #000080;\"><strong>How useful is twitter to you in terms of finding the right information?</strong></span></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![caption](https://github.com/demoonism/TwitterSensor/blob/master/Screenshot/search.JPG?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: center;\"><span style=\"text-decoration: underline;\"><span style=\"font-size: 20pt;\"><em><strong>We can do better than this!</strong> </em></span></span></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: left;\"><strong>In this project, we are aiming to train a classifier to identify targeted information on Twitter with high precision. </strong></p>\n",
    "<p style=\"text-align: left;\"><strong>For example, if you are interested in:</strong></p>\n",
    "<p style=\"text-align: left;\"><em><strong>&bull; Global social issues</strong></em><br /><em><strong>&bull; Politics in the Pacific Northwest</strong></em><br /><em><strong>&bull; Public transit in New York City</strong></em></p>\n",
    "<p style=\"text-align: left;\"><strong>The classifier would serve as a \"sensor\" to identify topical tweets based on your tailored interests!</strong></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><span style=\"color: #000080;\"><strong>Challenges</strong></span></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: left;\"><strong>(1) &nbsp;Billions of potential features, thousands of useful ones (Hashtags, users, mentions, terms, locations)</strong></p>\n",
    "<p style=\"text-align: left;\"><strong>(2) &nbsp;Need a lot of labeled data to learn feature weights well</strong></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><span style=\"color: #000080;\"><strong>Solution</strong></span></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><span style=\"font-size: 12pt;\"><strong>(1) Careful feature engineering and feature selection using Apache Spark.</strong></span></p>\n",
    "<p><span style=\"font-size: 10pt;\"><strong>We performed feature selection and transformation with Apache Spark on a standalone server with eight 1TB Hard disks, two 20 core CPU (40 threads) and 256GB RAM. </strong></span></p>\n",
    "<p><span style=\"font-size: 12pt;\"><strong>(2)</strong> <strong>Hashtags!</strong>&nbsp;</span></p>\n",
    "<p><span style=\"font-size: 10pt;\"><strong>Hashtags&nbsp;originated on IRC chat, were&nbsp;adopted later (and perhaps most famously) on Twitter, and&nbsp;now appear on other social media platforms such as Instagram,&nbsp;Tumblr, and Facebook. They usually serve as surogates for topics. Therefore, for each topic,&nbsp;we leverage a (small)&nbsp;set of user-curated topical hashtags to efficiently provide&nbsp;a large number of supervised topic labels for social media&nbsp;content.&nbsp;</strong></span></p>\n",
    "<p><span style=\"font-size: 10pt;\"><strong>We used 4 independent annotators to query the Twitter search API to identify candidate hashtags for each topic. A&nbsp;hashtag is assigned to a topic set if 3 out of 4 annotators agrees on the assignment.</strong></span></p>\n",
    "<p><span style=\"font-size: 10pt;\"><strong>For example, for the topic, \"Natural Disaster\", the set of hashtags are [\"sandy\", \"drought\", \"storm\", \"hurricane\", \"tornado\" .... etc]. If a tweet contains one or more of the pre-determined hashtags, we say it is \"topical\" for a particular toic, and it is labeled 1 (0 otherwise). We will revisit this in the feature selection section</strong></span></p>\n",
    "<p><span style=\"font-size: 10pt;\">&nbsp;</span></p>\n",
    "<p><span style=\"font-size: 18pt; color: #ff0000;\"><strong>Catch!</strong></span></p>\n",
    "<p><strong><span style=\"font-size: 10pt;\">Hashtag is part of our feature, wouldn't the classifier simply learn to remember the hashtag?</span></strong></p>\n",
    "<p><strong><span style=\"font-size: 10pt;\">To ensure maximum generality, we remove training hashtags from the validation and test set to ensure the classifier making prediction on the learnt feature and not just remembering hashtags. This would be further illlustrated in the Train-Validation split section later.</span></strong></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><span style=\"color: #000080;\"><strong>Now we have labeled data, what features could be useful for predciting topicality?</strong></span></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![caption](https://github.com/demoonism/TwitterSensor/blob/master/Screenshot/twt.JPG?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: left;\"><span style=\"font-size: 18pt; color: #000080;\"><strong>Why might these tweet features be useful?</strong> </span></p>\n",
    "<p style=\"text-align: left;\"><br /><span style=\"font-size: 10pt;\"><strong>&bull; Users: who tweets on the topic?</strong></span><br /><span style=\"font-size: 10pt;\"> <em><strong>-</strong></em> <span style=\"text-decoration: underline;\"><em><strong>Tweets from the weather channel might be a good indicator for Natural Disasters</strong></em></span></span></p>\n",
    "<p style=\"text-align: left;\"><br /><span style=\"font-size: 10pt;\"><strong>&bull; Hashtags: What hashtags co-occur with the topic?</strong></span><br /><span style=\"font-size: 10pt;\"> <em><strong>-</strong></em> <span style=\"text-decoration: underline;\"><em><strong>#teaparty could imply LBGT rights</strong></em></span></span></p>\n",
    "<p style=\"text-align: left;\"><br /><span style=\"font-size: 10pt;\"><strong>&bull; Mentions:</strong></span><br /><span style=\"font-size: 10pt;\"> <em><strong>-</strong></em> <span style=\"text-decoration: underline;\"><em><strong>@Redcross might be releavant to Natural Disaster</strong></em></span></span></p>\n",
    "<p style=\"text-align: left;\"><br /><span style=\"font-size: 10pt;\"><strong>&bull; Locations:</strong></span><br /><span style=\"font-size: 10pt;\"> <em><strong>-</strong></em> <span style=\"text-decoration: underline;\"><em><strong>Philippines where a lot of natural disaster happend in the last few years is a descent guess for releavant topics</strong></em></span></span></p>\n",
    "<p style=\"text-align: left;\"><br /><span style=\"font-size: 10pt;\"><strong>&bull; Terms:</strong></span><br /><span style=\"font-size: 10pt;\"> <em><strong>-</strong></em> <span style=\"text-decoration: underline;\"><em><strong> Word features are strong indicators of a particular topic</strong></em></span></span></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><span style=\"color: #000080;\"><strong>Implementation</strong></span></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><strong><span style=\"font-size: 10pt;\">The original Twitter data were collected over 2 years, which contains over 2TB compressed data. It consists of hundreds of millions lines of tweets.</span></strong></p>\n",
    "<p><strong><span style=\"font-size: 10pt;\">How do we go from the raw data to an efficient classifier?</span></strong></p>\n",
    "<p><strong><span style=\"font-size: 10pt;\">The following three-step processes serves an end-to-end pipeline to perform ETL and ML training.</span></strong></p>\n",
    "<p>&nbsp;</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><span style=\"color: #000080;\"><strong><span style=\"font-family: georgia, palatino, serif; font-size: 24pt;\">Step One: Pre-Processing</span></strong></span></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><span style=\"font-size: 13.3333px;\"><strong>Each valid tweet crawled from the server is a json object with over 100 attributes. An example could be find as following:</strong></span></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><span style=\"font-size: 10pt;\"><strong>Sample Tweet</strong></span></p>\n",
    "<p><span style=\"font-size: 8pt;\"><strong>{</strong>\"created_at\":\"Thu Jan 31 12:58:06 +0000 2013\",</span><br /><span style=\"font-size: 8pt;\"> \"id\":296965581582786560,</span><br /><span style=\"font-size: 8pt;\"> \"id_str\":\"296965581582786560\",</span><br /><span style=\"font-size: 8pt;\"> \"text\":\"Im ready for whatever\",</span><br /><span style=\"font-size: 8pt;\"> \"source\":\"\\u003ca href=\\\"http:\\/\\/twitter.com\\/download\\/iphone\\\" rel=\\\"nofollow\\\"\\u003eTwitter for iPhone\\u003c\\/a\\u003e\",</span><br /><span style=\"font-size: 8pt;\"> \"truncated\":false,</span><br /><span style=\"font-size: 8pt;\"> \"in_reply_to_status_id\":null,</span><br /><span style=\"font-size: 8pt;\"> \"in_reply_to_status_id_str\":null,</span><br /><span style=\"font-size: 8pt;\"> \"in_reply_to_user_id\":null,</span><br /><span style=\"font-size: 8pt;\"> \"in_reply_to_user_id_str\":null,</span><br /><span style=\"font-size: 8pt;\"> \"in_reply_to_screen_name\":null,</span><br /><span style=\"font-size: 8pt;\"> \"user\":{</span><br /><span style=\"font-size: 8pt;\"> \"id\":1059349532,</span><br /><span style=\"font-size: 8pt;\"> \"id_str\":\"1059349532\",</span><br /><span style=\"font-size: 8pt;\"> \"name\":\"Don Dada\",</span><br /><span style=\"font-size: 8pt;\"> \"screen_name\":\"ImDatNiggaBD\",</span><br /><span style=\"font-size: 8pt;\"> \"location\":\"South Side Of Little Rock\",</span><br /><span style=\"font-size: 8pt;\"> \"url\":null,</span><br /><span style=\"font-size: 8pt;\"> \"description\":\"Weed Smoker (Kush)\",</span><br /><span style=\"font-size: 8pt;\"> \"protected\":false,</span><br /><span style=\"font-size: 8pt;\"> \"followers_count\":109,</span><br /><span style=\"font-size: 8pt;\"> \"friends_count\":110,</span><br /><span style=\"font-size: 8pt;\"> \"listed_count\":0,</span><br /><span style=\"font-size: 8pt;\"> \"created_at\":\"Fri Jan 04 02:37:28 +0000 2013\",</span><br /><span style=\"font-size: 8pt;\"> \"favourites_count\":14,</span><br /><span style=\"font-size: 8pt;\"> \"utc_offset\":null,</span><br /><span style=\"font-size: 8pt;\"> \"time_zone\":null,</span><br /><span style=\"font-size: 8pt;\"> \"geo_enabled\":false,</span><br /><span style=\"font-size: 8pt;\"> \"verified\":false,</span><br /><span style=\"font-size: 8pt;\"> \"statuses_count\":1312,</span><br /><span style=\"font-size: 8pt;\"> \"lang\":\"en\",</span><br /><span style=\"font-size: 8pt;\"> \"contributors_enabled\":false,</span><br /><span style=\"font-size: 8pt;\"> \"is_translator\":false,</span><br /><span style=\"font-size: 8pt;\"> \"profile_background_color\":\"C0DEED\",</span><br /><span style=\"font-size: 8pt;\"> \"profile_background_image_url\":\"http:\\/\\/a0.twimg.com\\/images\\/themes\\/theme1\\/bg.png\",</span><br /><span style=\"font-size: 8pt;\"> \"profile_background_image_url_https\":\"https:\\/\\/si0.twimg.com\\/images\\/themes\\/theme1\\/bg.png\",</span><br /><span style=\"font-size: 8pt;\"> \"profile_background_tile\":false,</span><br /><span style=\"font-size: 8pt;\"> \"profile_image_url\":\"http:\\/\\/a0.twimg.com\\/profile_images\\/3184813228\\/d6d3a95d902f088f412cf1bd90c126c7_normal.jpeg\",</span><br /><span style=\"font-size: 8pt;\"> \"profile_image_url_https\":\"https:\\/\\/si0.twimg.com\\/profile_images\\/3184813228\\/d6d3a95d902f088f412cf1bd90c126c7_normal.jpeg\",</span><br /><span style=\"font-size: 8pt;\"> \"profile_banner_url\":\"https:\\/\\/si0.twimg.com\\/profile_banners\\/1059349532\\/1359068332\",</span><br /><span style=\"font-size: 8pt;\"> \"profile_link_color\":\"0084B4\",</span><br /><span style=\"font-size: 8pt;\"> \"profile_sidebar_border_color\":\"C0DEED\",</span><br /><span style=\"font-size: 8pt;\"> \"profile_sidebar_fill_color\":\"DDEEF6\",</span><br /><span style=\"font-size: 8pt;\"> \"profile_text_color\":\"333333\",</span><br /><span style=\"font-size: 8pt;\"> \"profile_use_background_image\":true,</span><br /><span style=\"font-size: 8pt;\"> \"default_profile\":true,</span><br /><span style=\"font-size: 8pt;\"> \"default_profile_image\":false,</span><br /><span style=\"font-size: 8pt;\"> \"following\":null,</span><br /><span style=\"font-size: 8pt;\"> \"follow_request_sent\":null,</span><br /><span style=\"font-size: 8pt;\"> \"notifications\":null},</span><br /><span style=\"font-size: 8pt;\"> \"geo\":null,</span><br /><span style=\"font-size: 8pt;\"> \"coordinates\":null,</span><br /><span style=\"font-size: 8pt;\"> \"place\":null,</span><br /><span style=\"font-size: 8pt;\"> \"contributors\":null,</span><br /><span style=\"font-size: 8pt;\"> \"retweet_count\":0,</span><br /><span style=\"font-size: 8pt;\"> \"entities\":{\"hashtags\":[],</span><br /><span style=\"font-size: 8pt;\"> \"urls\":[],</span><br /><span style=\"font-size: 8pt;\"> \"user_mentions\":[]},</span><br /><span style=\"font-size: 8pt;\"> \"favorited\":false,</span><br /><span style=\"font-size: 8pt;\"> \"retweeted\":false,</span><br /><span style=\"font-size: 8pt;\"> \"lang\":\"en\"<strong>}</strong></span></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><span style=\"font-size: 13.3333px;\"><strong>Obviously, not all attributes are relevant to our analysis. In the context of this paper, the only releavant fields in our features are:</strong></span></p>\n",
    "<p><span style=\"color: #0000ff;\"><em><strong>Hashtags, From_User, Create_Time, Location, Mentions</strong></em></span></p>\n",
    "<p><span style=\"font-size: 13.3333px;\"><strong>Moreover, the raw text is quite dirty. We need to perform some data cleaning in order to get proper features.</strong></span></p>\n",
    "<p><span style=\"font-size: 13.3333px;\"><strong>Since is step is fairly involveda and independet of the analysis here, I keep them in a separate Notebook. </strong></span></p>\n",
    "<blockquote>\n",
    "<p><span style=\"font-size: 13.3333px;\"><strong>Spark-Twt-PreProcessing.ipynb</strong></span></p>\n",
    "</blockquote>\n",
    "<p><span style=\"font-size: 13.3333px;\"><strong> You should be able to follow along as an indepent module.</strong></span></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><span style=\"font-size: 13.3333px;\"><strong>The resulting data looks like this:</strong></span></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><strong>Processed-tweet:</strong></p>\n",
    "<p><strong>{</strong>u'Create_time': 1359737884.0,<br /> u'from_id': 87151732,<br /> u'from_user': u'ishiPTI',<br /> u'hashtag': u'thuglife',<br /> u'location': u'loc_lakeshore',<br /> u'mention': u'BushraShekhani',<br /> u'term': u'I am ready for whatever',<br /> u'tweet_id': 297312861586325504<strong>}</strong></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><span style=\"font-size: 13.3333px;\"><strong>Now we have a small (sort of) and clean dataset to work with, it is time to move on to spark to perform some reall analysis.</strong></span></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><span style=\"color: #000080;\"><strong><span style=\"font-family: georgia, palatino, serif; font-size: 24pt;\">Step Two: Feature Extraction</span></strong></span></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><span style=\"font-size: 13.3333px;\"><strong>We need to turn the raw json data into a feature matrix. There are two keys here: </strong></span></p>\n",
    "<p><span style=\"font-size: 13.3333px;\"><strong>1. Data processing must be extremly efficient since we only have 40 cores and 256G ram.</strong></span></p>\n",
    "<p><span style=\"font-size: 13.3333px;\"><strong>2. The resulting matrix must be sparse to facilitate the training step&nbsp;later.</strong></span></p>\n",
    "<p><span style=\"font-size: 13.3333px;\"><strong>These are achieved through the following pipeline. </strong></span></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Step 1\n",
    "\n",
    "## Notebook property setup.\n",
    "## Spark SQL\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import udf, col, lit, monotonically_increasing_id, explode\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "## Spark ML\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.ml.classification import LogisticRegression, DecisionTreeClassifier\n",
    "from pyspark.ml.param import Param, Params\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer, IDF, StopWordsRemover, CountVectorizer, VectorAssembler\n",
    "\n",
    "## Helper\n",
    "import re\n",
    "import string\n",
    "import sys\n",
    "import time\n",
    "import os.path\n",
    "import json\n",
    "from datetime import datetime\n",
    "from operator import add\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "## Enable inline graphs\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "## Display precision for pandas dataframe\n",
    "pd.set_option('precision',10)\n",
    "\n",
    "workdir_1e = \"/mnt/1e69d2b1-91a9-473c-a164-db90daf43a3d/Eng_Json/\"\n",
    "workdir_2b = \"/mnt/2b53fde0-61da-4eeb-a038-9910540ff9ad/Eng_Json/\"\n",
    "workdir_4e = \"/mnt/4e8ba653-f2f0-4e18-a51e-458026833dee/final_parquet\"\n",
    "workdir_66 = \"/mnt/66e695cd-1a0c-4e3b-9a50-55e01b788529/Training_data\"\n",
    "workdir_b9 = \"/mnt/b93e71ec-8ddf-4033-bd42-770c05bc68aa/Feature_Vector\"\n",
    "# Sample bash code to change folder access.\n",
    "# !chgrp danielshi /mnt/66e695cd-1a0c-4e3b-9a50-55e01b788529/Training_data/\n",
    "# !chmod g+s /mnt/b93e71ec-8ddf-4033-bd42-770c05bc68aa/Feature_Vector/\n",
    "# !setfacl -d -m g::rwx /mnt/b93e71ec-8ddf-4033-bd42-770c05bc68aa/Feature_Vector/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "## Helper function to keep track of the run time of a spark ops.\n",
    "def getTime(start):\n",
    "    sec = time.time() - start\n",
    "    m, s = divmod(sec, 60)\n",
    "    h, m = divmod(m, 60)\n",
    "    print('Spark operation takes - %d:%02d:%02d which is %d seconds in total' % (h,m,s,sec))\n",
    "    \n",
    "# load json object, if a line is invalid, substitute as an empty dict (which has len() == 0 )\n",
    "def loadJson(d):\n",
    "    try:\n",
    "        js = json.loads(d)\n",
    "    except ValueError as e:\n",
    "        js = {}\n",
    "    except Exception:\n",
    "        js = {}\n",
    "    return js\n",
    "\n",
    "def translating(x):\n",
    "    return x.encode('utf-8').lower().translate(None, string.punctuation)\n",
    "\n",
    "def loc_clean(d):\n",
    "\n",
    "    if d == None or d.strip(' ') == '':\n",
    "        loc_term = \"empty_location\"\n",
    "    else:\n",
    "        loc_term = 'loc_' + \"_\".join(map(translating, d.strip(' ').split(\" \")))\n",
    "        \n",
    "    return loc_term\n",
    "\n",
    "loc_udf = udf(loc_clean, StringType())\n",
    "\n",
    "\n",
    "def hash_clean(d):\n",
    "\n",
    "    if d == None or d.strip(' ') == '':\n",
    "        hashtags = \"empty_hashtag\"\n",
    "    else:\n",
    "        hashtags = d\n",
    "        \n",
    "    return hashtags\n",
    "\n",
    "hash_udf = udf(hash_clean, StringType())\n",
    "\n",
    "\n",
    "def mention_clean(d):\n",
    "\n",
    "    if d == None or d.strip(' ') == '':\n",
    "        mentions = \"empty_mention\"\n",
    "    else:\n",
    "        mentions = d\n",
    "        \n",
    "    return mentions\n",
    "\n",
    "mention_udf = udf(mention_clean, StringType())\n",
    "\n",
    "\n",
    "def clean_term(d):\n",
    "    tags = d['hashtag'].split()\n",
    "    user = d['from_user'].split()\n",
    "    mention = d['mention'].split()  \n",
    "    text = d['term'].encode('ascii', 'ignore')\n",
    "    for ppl in mention:\n",
    "        text = text.replace('@'+ppl, '')\n",
    "    for tag in tags:\n",
    "        text = text.replace('#'+tag, '')\n",
    "        \n",
    "    text = re.sub(r'(https?://\\S+)', '',text).replace(\":\", \"\").lower()\n",
    "\n",
    "\n",
    "    if text == None or text.strip(' ') == '':\n",
    "        terms = \"empty_tweet\"\n",
    "    else:\n",
    "        terms = \" \".join(text.encode('utf-8').translate(None, string.punctuation).strip().split())\n",
    "\n",
    "    updated = {'create_time': d['create_time'],\n",
    "      'from_id': d['from_id'],\n",
    "      'from_user': d['from_user'],\n",
    "      'hashtag': d['hashtag'],\n",
    "      'location': d['location'],\n",
    "      'mention': d['mention'],\n",
    "      'term': terms,\n",
    "      'tweet_id': d['tweet_id']}    \n",
    "    return updated\n",
    "\n",
    "\n",
    "def finalCLeaning(file_obj, output):\n",
    "    data_1 = file_obj.map(loadJson)\n",
    "    cleaned_dat = data_1.map(clean_term)\n",
    "    df_p1 = sqlContext.createDataFrame(cleaned_dat, schema)\n",
    "    df_p1.write.save(workdir_66+output, format=\"parquet\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><span style=\"font-size: 18px;\"><strong>Reading DATA </strong></span></p>\n",
    "<p><span style=\"font-size: 13.3333px;\"><strong>After preprocessing, data are saved as parquet fiels. We need to load and parse these data into dataframes. Note that, the sc.textFile function's input directory could be either a file or a directory. Spark context will create partitions automatically. Note that the pre-processed data are stored in two directories.</strong></span></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# full\n",
    "data_Eng = sc.textFile(workdir_1e+\", \"+workdir_2b)\n",
    "data = data_Eng.map(loadJson)\n",
    "# Taking a look at the (parsed) first line of our input files\n",
    "data.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><span style=\"color: #000080;\"><strong>Turning to dataframe</strong></span></h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><span style=\"font-size: 13.3333px;\"><strong>An RDD (Resilient Distributed Dataset) is more of a blackbox dataset that cannot be easily optimized as the operations that can be performed against it are not as constrained. (Available in Spark since 1.0) </strong></span></p>\n",
    "<p><span style=\"font-size: 13.3333px;\"><strong>A dataframe is a table, or two-dimensional array-like structure, in which each column contains measurements on one variable, and each row contains one case. Therefore, a DataFrame has additional metadata due to its tabular format, which allows Spark to run certain optimizations on the finalized query. (Added since 1.3) </strong></span></p>\n",
    "<p><span style=\"font-size: 13.3333px;\"><strong>In summary, you are able to write traditional map-reduce type of code on both RDD and Dataframe, but Dataframe also support SQL command and built-in analytical functions. For performance consideration, we are turning our RDD into Dataframes first.</strong></span></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Define Dataframe schema.\n",
    "schema = StructType([StructField('create_time', DoubleType(), False),\n",
    "                     StructField('from_id', StringType(), False),\n",
    "                     StructField('from_user', StringType(), False),\n",
    "                     StructField('hashtag', StringType(), True),\n",
    "                     StructField('location', StringType(), True),\n",
    "                     StructField('mention', StringType(), True),\n",
    "                     StructField('term', StringType(), True),\n",
    "                     StructField('tweet_id', StringType(), False)\n",
    "                    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = sqlContext.createDataFrame(data, schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><span style=\"font-size: 13.3333px;\"><strong>Saving dataframe to parquet format for easy loading. NOTE: you will see a lot of I/O code being commented out. These are intermeidte resultS used to produce other DFs later. We don't need to run them everytime; only run if you want to reproduce the result.</strong></span></p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#df.write.save(workdir_4e+\"/Eng_DF\", format=\"parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#df = spark.read.parquet(workdir_4e+\"/Eng_DF\")\n",
    "#df.show(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><span style=\"font-size: 13.3333px;\"><strong>Input are shown as tabular form (Dataframe) below. Note that hashtag field is happend to be null for the first few records. </strong></span></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<p><span style=\"font-size: 13.3333px;\"><strong>Clean the hashtag, mention, username and location by removing null and aun-wanted chars/punctuations. We utilized user-defined functions here. Essentially this is the same concept of apply a custom map operation on a column </strong></span></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clean_stage1 = df.withColumn(\"clean_loc\", loc_udf(df.location)).\\\n",
    "                withColumn(\"clean_hash\", hash_udf(df.hashtag)).\\\n",
    "                withColumn(\"clean_mention\", mention_udf(df.mention))\n",
    "clean_stage1.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#clean_stage1.write.save(workdir_4e+\"/Eng_DF_clean_stage1\", format=\"parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#clean_stage1 = spark.read.parquet(workdir_4e+\"/Eng_DF_clean_stage1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Map back to the old names\n",
    "Pre_cleansing_df = clean_stage1.select(clean_stage1.create_time, \\\n",
    "                   clean_stage1.from_id, \\\n",
    "                   clean_stage1.from_user, \\\n",
    "                   clean_stage1.tweet_id, \\\n",
    "                   clean_stage1.term, \\\n",
    "                   clean_stage1.clean_loc, \\\n",
    "                   clean_stage1.clean_hash, \\\n",
    "                   clean_stage1.clean_mention)\n",
    "\n",
    "clean_stage2 = Pre_cleansing_df.withColumnRenamed(\"clean_loc\", \"location\").withColumnRenamed(\"clean_hash\", \"hashtag\").withColumnRenamed(\"clean_mention\", \"mention\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#clean_stage2.write.json(workdir_66+\"/Eng_DF_clean_stage2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#clean_stage2 = sc.textFile(workdir_66+\"/Eng_DF_clean_stage2/*.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reframing\n",
    "# Perform cleaning for tweet terms column. This used to be the most time consuming step for processing, but after some\n",
    "# optimization it takes roughly the same time as the other cleaning steps.\n",
    "# Note that the convention followed here is more \"hadoop\" as intermediate steps are saved separately. Although not \n",
    "# necessary, this ensures that we have somewhere to fall back on if incurs problems at any of the stages \n",
    "\n",
    "finalCLeaning(clean_stage2, \"/Staging_final\")\n",
    "Stg_final = spark.read.parquet(workdir_66+\"/Staging_final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Stg_final.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<p><span style=\"font-size: 13.3333px;\"><strong>Now our dataframe is nice and clean, the next task is to label the dataset. Remember our criteria is that a tweet is topical if it contains one of our pre-defined hashtag list for a given topic. Note that a tweet could contain multiple hashtags, if one of them is releavant, we would consider it as releavant. Therefore, what we need to do here is to \"flat-out\" the the hashtag list, find an unique list of tweet ids which contains releavant hashtags, and then join it back to the original DF.</strong></span></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Define topical hashtag list\n",
    "topic_dict = {\n",
    "    \"Soccer\":{\"soccer\", \"football\", \"worldcup\", \"sports\", \"futbol\", \"fifa\", \"mls\", \"worldcup2014\", \"epl\", \"sportsroadhouse\", \"sport\", \"adidas\", \"messi\", \"usmnt\", \"arsenal\", \"manchesterunited\", \"nike\", \"ronaldo\", \"manutd\", \"fifaworldcup\", \"foot\", \"ussoccer\", \"sportsbetting\", \"realmadrid\", \"aleague\", \"chelsea\", \"manchester\", \"cr7\", \"footballnews\", \"championsleague\", \"youthsoccer\", \"eplleague\", \"barcelona\", \"brazil2014\", \"soccerproblems\", \"premierleague\", \"brasil2014\", \"soccerlife\", \"cristianoronaldo\", \"uefa\", \"fifa2014\", \"beckham\", \"fifa14\", \"neymar\", \"fussball\", \"soccergirls\", \"barca\", \"manchestercity\", \"league\", \"fútbol\", \"halamadrid\", \"bayern\", \"women\", \"lfc\", \"goalkeeper\", \"everton\", \"bayernmunich\", \"soccerprobs\", \"league1\", \"juventus\", \"nufc\", \"mcfc\", \"cristiano\", \"eurosoccercup\", \"platini\", \"socce\", \"mancity\", \"torontofc\", \"dortmund\", \"derbyday\", \"fifa15\", \"liverpool\", \"league2\", \"ilovesoccer\", \"fcbarcelona\", \"maradona\", \"intermilan\", \"futebol\", \"soccergirlprobs\", \"soccersixfanplayer\", \"realfootball\", \"gunners\", \"confederationscup\", \"worldcupproblems\", \"ballondor\", \"collegesoccer\", \"rooney\", \"flagfootball\", \"realsaltlake\", \"lionelmessi\", \"usavsportugal\", \"europaleague\", \"soccernews\", \"uefachampionsleague\", \"psg\", \"gobrazil\", \"uslpro\", \"wc2014\", \"suarez\", \"bvb\", \"soccerprobz\", \"worldcupqualifiers\", \"torres\", \"footbal\", \"balotelli\", \"nashville\", \"inter\", \"milano\", \"cardiff\", \"jleague\", \"nwsl\", \"ozil\", \"worldcup2014brazil\", \"nycfc\", \"mess\", \"soccernation\", \"pelé\", \"tottenham\", \"ligue1\", \"landondonovan\", \"atletico\", \"worldcup14\", \"torino\", \"soccerislife\", \"fernandotorres\", \"ronaldinho\", \"goldenball\", \"wembley\", \"brazilvscroatia\", \"collegefootball\", \"elclassico\", \"footba\", \"fifa13\", \"soccersunday\", \"englandsoccercup\", \"usasoccer\", \"womensfootball\", \"fcbayern\", \"fifaworldcup2014\", \"usavsgermany\", \"neymarjr\", \"soccersucks\", \"arturovidal\", \"zidane\", \"ballislife\", \"usavsger\", \"mlscup\", \"worldcupfinal\", \"ajax\", \"soccerball\", \"lovesoccer\", \"euro2013\", \"soccergame\", \"premiereleague\", \"mu\", \"lionel\", \"soccermanager\", \"mundial2014\", \"portugalvsgermany\", \"soccerseason\", \"mondiali2014\", \"davidbeckham\", \"redbulls\", \"argvsned\", \"selecao\", \"usavsmex\", \"soccergirlproblems\", \"soccerlove\", \"2014worldcup\", \"soccergrlprobs\", \"germanyvsargentina\", \"zlatan\", \"napoli\", \"muller\", \"confederations_cup\", \"championsleaguefinal\", \"worldcuppredictions\", \"clasico\", \"liverpoolvsrealmadrid\", \"mundialsub17\", \"worldcupbrazil\", \"leaguechamps\", \"arsenalfans\", \"germanyvsalgeria\", \"netherlandsvsargentina\", \"belvsusa\", \"bravsned\", \"mexicovsusa\", \"englandvsuruguay\", \"germanyvsbrazil\", \"brazilvsnetherlands\", \"gervsarg\", \"engvsita\", \"brazilvsgermany\", \"englandvsitaly\", \"espvsned\", \"crcvsned\", \"ghanavsusa\", \"francevsswitzerland\", \"argentinavsgermany\", \"spainvsnetherlands\", \"usavscan\", \"worldcupbrazil2014\", \"brazil2014worldcup\", \"fifaworldcupbrazil\", \"worldcup2018\", \"championleague\"},\n",
    "    \"Natr_Disaster\":{\"sandy\", \"drought\", \"storm\", \"hurricane\", \"tornado\", \"hurricanesandy\", \"earthquake\", \"arthur\", \"julio\", \"manuel\", \"flood\", \"hurricanes\", \"quakelive\", \"hurricaneseason\", \"hurricaneseason\", \"hurricanepride\", \"quake\", \"hurricanekatrina\", \"katrina\", \"floodwarning\", \"eqnz\", \"bertha\", \"tsunami\", \"tsunamimarch\", \"hurricanekid\", \"drought3\", \"hurricanenia\", \"hurricanenation\", \"cholera\", \"hurricanefly\", \"drought13\", \"laquake\", \"typhoon\", \"tsunami2004\", \"ukstorm\", \"hurricaneforever\", \"quakecon2013\", \"prayforchina\", \"quakecon\", \"manuelpellegrini\", \"flood2013\", \"prayforthephilippines\", \"hurricanepreparedness\", \"hurricaneharbor\", \"typhoons\", \"hurricane13\", \"abfloods\", \"ukfloods\", \"hurricaneweek\", \"typhoonmaring\", \"odile\", \"hurricaneprep\", \"phailin\", \"earthquakeph\", \"visayasquake\", \"haiyan\", \"typhoonyolanda\", \"typhoonhaiyan\", \"typhoonaid\", \"typhoonjet\", \"corkfloods\", \"laearthquake\", \"quakecon2014\", \"flood2014\", \"prayforchile\", \"chileearthquake\", \"serbiafloods\", \"tsunamihitsfaisalabad\", \"hurricanearthur\", \"tsunami4nayapakistan\", \"typhoonglenda\", \"hurricanebertha\", \"hurricaneiselle\", \"napaquake\", \"napaearthquake\", \"hurricanemarie\", \"kashmirfloods\", \"hurricaneodile\", \"hurricanegonzalo\", \"hurricaneana\", \"haiyan1year\", \"typhoonhagupit\", \"typhoonruby\"},\n",
    "    \"Health\": {\"health\", \"uniteblue\", \"ebola\", \"healthcare\", \"depression\", \"hiv\", \"cdc\", \"crisis\", \"obesity\", \"aids\", \"nurse\", \"flu\", \"alert\", \"publichealth\", \"bandaid30\", \"malaria\", \"disease\", \"fever\", \"antivirus\", \"virus\", \"lagos\", \"unsg\", \"sierraleone\", \"ebolaresponse\", \"ebolaoutbreak\", \"chanyeolvirusday\", \"aids2014\", \"vaccine\", \"mer\", \"homeopathy\", \"msf\", \"allergy\", \"nih\", \"humanitarianheroes\", \"stopthespread\", \"dengue\", \"flushot\", \"epidemic\", \"ebolainatlanta\", \"tuberculosis\", \"westafrica\", \"quarantine\", \"ebolavirus\", \"viruses\", \"kacihickox\", \"emory\", \"meningitis\", \"ebolaczar\", \"enterovirus\", \"pandemic\", \"stopebola\", \"chikungunya\", \"eplague\", \"childhoodobesity\", \"plague\", \"allergyseason\", \"coronavirus\", \"healthworkers\", \"endebola\", \"ebolaqanda\", \"obola\", \"h1n1\", \"aidsfree\", \"factsnotfear\", \"ebolafacts\", \"chickenpox\", \"birdflu\", \"ebolainnyc\", \"dallasebola\", \"ebolachat\", \"eboladallas\", \"childobesity\", \"healthsystems\", \"aidsday\", \"truedepressioniswhen\", \"askebola\", \"depressionawareness\", \"ambervinson\", \"depressionhurts\", \"ninapham\", \"nursesfightebola\", \"mickeyvirus\", \"rotavirus\", \"blackdeath\", \"theplague\"},\n",
    "    \"Social_issue\": {\"racism\", \"mikebrown\", \"shutitdown\", \"icantbreathe\", \"ferguson\", \"nojusticenopeace\", \"moa\", \"policebrutality\", \"antoniomartin\", \"thesystemisbroken\", \"justice4all\", \"michaelbrown\", \"blacklivesmatter\", \"blackxmas\", \"ericgarner\", \"justiceformikebrown\", \"handsupdontshoot\", \"alllivesmatter\", \"thisstopstoday\", \"fergusondecision\", \"tamirrice\", \"policelivesmatter\", \"berkeleyprotests\", \"millionsmarchnyc\", \"aurarosser\", \"nypdlivesmatter\", \"abortion\", \"debt\", \"gunlaws\", \"legalize\", \"legalizemarijuana\", \"nationaldebt\", \"abortions\", \"debts\", \"endabortion\", \"debtceiling\", \"legalizecannabis\", \"legalweed\", \"stopabortion\", \"legalized\", \"freetheweed\", \"abortionaccess\", \"abortionismurder\", \"newnjgunlaws\", \"newnjgunlaw\", \"abortionvote\", \"44millionabortions\", \"safeabortion\", \"legalize420\", \"nonewnjgunlaws\"},\n",
    "    \"Cele_death\":{\"jamesavery\", \"freshprince\", \"unclephil\", \"freshprinceofbelair\", \"rip\", \"ripjamesavery\", \"thefreshprinceofbelair\", \"robinwilliams\", \"nelsonmandela\", \"philipseymourhoffman\", \"paulwalker\", \"mandela\", \"prayforap\", \"madiba\", \"mayaangelou\", \"rippaulwalker\", \"riprobinwilliams\", \"ripnelsonmandela\", \"ripcorymonteith\", \"ripmandela\", \"ripjoanrivers\", \"riptalia\", \"riplilsnupe\", \"ripleerigby\", \"riprise\", \"ripmaeyoung\", \"ripshain\", \"ripeunb\", \"riposcardelarenta\", \"riplarryshippers\", \"ripkelcey\", \"riptitovilanova\", \"ripsimone\", \"riptrayvonmartin\", \"ripmayaangelou\", \"ripmadiba\", \"ripallisonargent\", \"ripunclephil\", \"ripmitchlucker\", \"riprogerebert\", \"ripjamesfoley\", \"ripshaingandee\", \"ripphilipseymourhoffman\", \"riplaurenbacall\"},\n",
    "    \"Iran\":{\"irantalks\", \"rouhani\", \"iranian\", \"irantalksvienna\", \"nonucleariran\", \"irannews\", \"irandeal\", \"irantalksnyc\", \"iranfreedom\", \"irani\", \"nuclearweapons\", \"irantalksoman\", \"irantalk\", \"nuclearenergy\", \"iranhrviolations\", \"iranianssupport\", \"nuclearpower\"},\n",
    "    \"Space\":{\"1yearonmars\", \"aerospace\", \"aliens\", \"antares\", \"apollo\", \"apollo11\", \"apollo13\", \"apollo45\", \"armstrong\", \"asknasa\", \"asteroid\", \"asteroids\", \"astr\", \"astro\", \"astrobiology\", \"astrology\", \"astronaut\", \"astronauts\", \"astronomy\", \"atlantis\", \"auroras\", \"blackhole\", \"blackholefriday\", \"blackholes\", \"bloodmoon\", \"bloodmooneclipse\", \"bluemoon\", \"bluemoontourenchile\", \"cassini\", \"clubpluto\", \"comet\", \"cometlanding\", \"comets\", \"cosmos\", \"curiosity\", \"cygnus\", \"darksideofthemoon\", \"discovery\", \"earth\", \"earthday\", \"earthrightnow\", \"eft1\", \"exoplanets\", \"exp40\", \"exp41\", \"extraterrestrial\", \"flight\", \"fullmoon\", \"fullmoonparty\", \"gagainspace2015\", \"get2space\", \"gocomets\", \"gravity\", \"harvestmoon\", \"houston\", \"houstonwehaveaproblem\", \"hubble\", \"inspace\", \"internationalspacestation\", \"interstellar\", \"iris\", \"isee3\", \"iss\", \"isscrew\", \"journeytomars\", \"jupiter\", \"kepler\", \"killthemoon\", \"ladee\", \"landsat\", \"livefromspace\", \"lunar\", \"lunareclipse\", \"mars\", \"marsiscoming\", \"marsmission\", \"marte\", \"maven\", \"meteor\", \"meteorgarden\", \"meteorite\", \"meteorites\", \"meteorito\", \"meteorjs\", \"meteorology\", \"meteors\", \"meteorshower\", \"meteorwatch\", \"missiontomars\", \"moon\", \"moonday\", \"moonlanding\", \"moonlight\", \"moons\", \"nasa\", \"nasasocial\", \"nasatv\", \"nasatweetup\", \"newmoon\", \"nextgiantleap\", \"orb3\", \"orion\", \"orionlaunch\", \"outerspace\", \"perseidmeteorshower\", \"planet\", \"planetearth\", \"planets\", \"planetsunburn\", \"pluto\", \"projectloon\", \"redmoon\", \"rocket\", \"rockets\", \"russianmeteor\", \"satellite\", \"satellites\", \"saturn\", \"science\", \"scientist\", \"scientists\", \"scifi\", \"scifinow\", \"solar\", \"solarsystem\", \"space\", \"spacebound\", \"spacecraft\", \"spaceinvaders\", \"spacelive\", \"spaceman\", \"spacemigrationtour\", \"spaces\", \"spaceship\", \"spaceshiptwo\", \"spacestation\", \"spacetoground\", \"spacetravel\", \"spacewalk\", \"spaceweather\", \"spacex\", \"spacex3\", \"stars\", \"starship\", \"startrek\", \"starwars\", \"stem\", \"sun\", \"supermoon\", \"supermoon2014\", \"supernova\", \"sxsw\", \"telescope\", \"themoon\", \"thirtysecondstomars\", \"universe\", \"upintheair\", \"venus\", \"visitjsc\", \"votemars\", \"voyager1\"},\n",
    "    \"Tennis\":{\"usopenxespn\", \"vansusopen\", \"womensausopen\", \"usopen\", \"usopen13\", \"usopen14\", \"usopen201\", \"usopen2013\", \"vansusopen\", \"usopen2014\", \"usopenchampion\", \"usopencup\", \"usopenfinal\", \"djokovic\", \"djokovicvsfederer\", \"djokovicvsmurray\", \"federervsdjokovic\", \"nadaldjokovic\", \"novakdjokovic\", \"teamdjokovic\", \"novak\", \"teamnovak\", \"frenchopen\", \"frenchopen2013\", \"frenchopen2014\", \"frenchopenfinal\", \"frenchopentennis\", \"australianopen\", \"australianopen2014\", \"atptennis\", \"espntennis\", \"lovetennis\", \"niketennis\", \"tennis\", \"afcwimbledon\", \"bbcwimbledon\", \"espnwimbledon\", \"lovewimbledon\", \"sendmetowimbledon\", \"wearewimbledon\", \"wimbledonfinal\", \"wimbledon\", \"wimbledontennis\", \"wimbledonxespn\", \"wimbledon13\", \"wimbledon2013\", \"wimbledon2014\", \"wimbledon2o13\", \"wimbledonchamp\", \"wimbledonchampion\", \"wimbledone\", \"wimbledonfinal2013\", \"wimbledonfinals\", \"whenandywonwimbledon\", \"atpmadrid\", \"atpmasters\", \"atpmontecarlo\", \"atptennis\", \"atpsunday\", \"atptour\", \"atptourfinals\", \"atpworldtour\", \"atpworldtourfinal\", \"atpworldtourfinals\", \"usopenseries\", \"usopentennis\", \"federervsnadal\", \"murraynadal\", \"nadal\", \"nadaldjokovic\", \"nadalfederer\", \"nadalferrer\", \"rafaelnadal\", \"rafanadal\", \"rafanadaltour\", \"teamnadal\", \"vamosnadal\", \"womenstennis\", \"canadiantennis\", \"chutennis\", \"tenniscanada\", \"cincytennis\", \"tennischannel\", \"collegetennis\", \"tenniscourt\", \"dubaitennis\", \"tenniscourts\", \"eurosporttennis\", \"tenniselbow\", \"tennisiscanada\", \"tennisnews\", \"sydneytennis\", \"teamfrancetennis\", \"tennisball\"},\n",
    "    \"Human_Disaster\":{\"syria\", \"gaza\", \"isis\", \"israel\", \"mh370\", \"gazaunderattack\", \"mh17\", \"palestine\", \"freepalestine\", \"is\", \"bringbackourgirls\", \"prayforgaza\", \"iss\", \"hamas\", \"prayformh370\", \"isil\", \"taliban\", \"syrian\", \"southsudan\", \"bds\", \"icc4israel\", \"younusalgohar\", \"israeli\", \"palestinian\", \"idf\", \"malala\", \"malaysiaairlines\", \"sudan\", \"bokoharam\", \"palestinians\", \"jamesfoley\", \"jamesfoley\", \"chibokgirls\", \"daesh\", \"alqaeda\", \"childrenofsyria\", \"ajagaza\", \"rafah\", \"notinmyname\", \"gazaunderfire\", \"freesyria\", \"withsyria\", \"abuja\", \"nowarwithsyria\", \"farc\", \"ripmh370\", \"drugwar\", \"syriawarcrimes\", \"stopwar\", \"bombsquad\", \"handsoffsyria\", \"malnutrition\", \"chibok\", \"juba\", \"bringourgirlsback\", \"southsudannow\", \"whereisthefuckingplane\", \"cholera\", \"antiwar\", \"realsyria\", \"savesyria\", \"isismediablackout\", \"alshabab\", \"iraqwar\", \"nigerianschoolgirls\", \"ripjamesfoley\", \"famine\", \"bronxbombers\", \"bringbackourdaughters\", \"igad\", \"bringbackourgirl\", \"helpsyriasrefugees\", \"bostonmarathonbombing\", \"redefinenigeria\", \"234whitegirls\", \"bombthreat\", \"stayoutofsyria\", \"bentiu\"},\n",
    "    \"LGBT\":{\"tcot\", \"p2\", \"pjnet\", \"uniteblue\", \"teaparty\", \"2a\", \"ccot\", \"equality\", \"marriageequality\", \"tgdn\", \"pride\", \"stoprush\", \"loveislove\", \"popefrancis\", \"vatican\", \"legalizeit\", \"gaymarriage\", \"legalize\", \"wapol\", \"homo\", \"equality4all\", \"ssm\", \"ibdeditorials\", \"gaypride\", \"equalityforall\", \"wakeupamerica\", \"samesexmarriage\", \"lovewins\", \"homosexuality\", \"ally\", \"homosexual\", \"alliances\", \"equalitymatters\", \"marylandpride\", \"legalizegayma\", \"homos\", \"acceptancematters\", \"gaylove\", \"sacksheila\", \"gaymoment\", \"equalityformen\", \"unitebluemt\", \"gaymen\", \"sacks\", \"equalitynow\", \"legalizegay\"}\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><span style=\"font-size: 13.3333px;\"><strong>Each tweet could contain multiple hashtags, we need to normalize this attribute. This will facilitate the labeling step later.</strong></span></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"hashtag\", outputCol=\"each_hashtag\")\n",
    "hashtags_df = tokenizer.transform(Stg_final)\n",
    "\n",
    "hashtag =  hashtags_df.select(\"tweet_id\",\"create_time\",\"each_hashtag\")\n",
    "hash_exploded = hashtag.withColumn('each_hashtag', explode('each_hashtag'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#hash_exploded.write.save(workdir_4e+\"/hash_exploded\", format=\"parquet\")\n",
    "hash_exploded = spark.read.parquet(workdir_4e+\"/hash_exploded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><span style=\"font-size: 13.3333px;\"><strong>If a hashtag is in the predefined list, we mark the corresponding tweet as topical. Using distinct ops to get a unique list of topical id for a particular topic.  </strong></span></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Tennis\")\n",
    "print(\"num hastag: \" + len(topic_dict[\"Tennis\"]))\n",
    "tennis_topical_ids = hash_exploded.select(hash_exploded.tweet_id).where(hash_exploded.each_hashtag.isin(topic_dict[\"Tennis\"])).distinct().cache()\n",
    "#print(topic_ids.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><span style=\"font-size: 13.3333px;\"><strong> Now we have a list of tweets ids that are topical for tennis</strong></span></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tennis_topical_ids.write.save(workdir_66+\"/tennis_topical_ids\", format=\"parquet\")\n",
    "tennis_topical_ids = spark.read.parquet(workdir_66+\"/tennis_topical_ids\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tennis_topical_ids.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><span style=\"font-size: 13.3333px;\"><strong> Join the ids back to obtain the full label</strong></span></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getLabeledDf(df_topic):\n",
    "    Labeled_topical = df_topic.withColumn(\"topical\", lit(1))\n",
    "    Labled_df = Stg_final.join(Labeled_topical, Stg_final.tweet_id == Labeled_topical.tweet_id, \"left\").\\\n",
    "                                       select(Stg_final.create_time,\\\n",
    "                                              Stg_final.from_id,\\\n",
    "                                              Stg_final.from_user,\\\n",
    "                                              Stg_final.hashtag,\\\n",
    "                                              Stg_final.location,\\\n",
    "                                              Stg_final.mention,\\\n",
    "                                              Stg_final.tweet_id,\\\n",
    "                                              Stg_final.term,\\\n",
    "                                              F.when(Labeled_topical.topical == 1, 1.0).otherwise(0.0).alias(\"label\")).distinct()\n",
    "    \n",
    "    return Labled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Labled_df = getLabeledDf(tennis_topical_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Labled_df.write.save(workdir_4e+\"/Labled_df\", format=\"parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Labled_df = spark.read.parquet(workdir_4e+\"/Labled_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tennis_labels = Labled_df.select(\"label\",\"tweet_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><span style=\"font-size: 13.3333px;\"><strong>Saving the full label column for later use.</strong></span></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#tennis_labels.write.save(workdir_66+\"/tennis_topical_labels\", format=\"parquet\")\n",
    "tennis_labels = spark.read.parquet(workdir_66+\"/tennis_topical_labels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><span style=\"color: #000080;\"><strong>Vectorizing user, hashtag, location, mention, term into feature vectors</strong></span></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><span style=\"font-size: 13.3333px;\"><strong>We have an overwhelming number of features; it is essentail to threshold them to avoid overfitting. We use the same threshold as describbed in the paper. Note that the threshold is for DF, not TF.</strong></span></p>\n",
    "![caption](https://github.com/demoonism/TwitterSensor/blob/master/Screenshot/featurecount.JPG?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><span style=\"font-size: 13.3333px;\"><strong>In this section, we vectorize each feature according to the count threshold above. </strong></span></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><span style=\"font-size: 13.3333px;\"><strong>Term Feature Threshold. Removing stop wprds first, and only take feature with df count > 50 </strong></span></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "term_tokenizer = Tokenizer(inputCol=\"term\", outputCol=\"words\")\n",
    "term_remover = StopWordsRemover(inputCol=term_tokenizer.getOutputCol(), outputCol=\"filtered\")\n",
    "term_cv = CountVectorizer(inputCol=term_remover.getOutputCol(), outputCol=\"term_features\", minDF=50)\n",
    "pipeline_term = Pipeline(stages=[term_tokenizer,term_remover,term_cv])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loading = time.time()\n",
    "\n",
    "model = pipeline_term.fit(clean_data)\n",
    "Feat_term = model.transform(clean_data).select(\"term_features\", \"tweet_id\")\n",
    "\n",
    "getTime(loading)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Feat_term.write.save(workdir_4e+\"/Feature_term\", format=\"parquet\")\n",
    "#Feat_term = spark.read.parquet(workdir_4e+\"/Feature_term\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(model.vocabulary)\n",
    "#http://stackoverflow.com/questions/32285699/how-to-get-word-details-from-tf-vector-rdd-in-spark-ml-lib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><span style=\"font-size: 13.3333px;\"><strong>Hashtag Feature Threshold. </strong></span></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hashtag_tokenizer = Tokenizer(inputCol=\"hashtag\", outputCol=\"tags\")\n",
    "hashtag_cv = CountVectorizer(inputCol=hashtag_tokenizer.getOutputCol(), outputCol=\"hashtag_features\", minDF=159)\n",
    "pipeline_hashtag = Pipeline(stages=[hashtag_tokenizer,hashtag_cv])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loading = time.time()\n",
    "\n",
    "model = pipeline_hashtag.fit(clean_data)\n",
    "Feat_hashtag = model.transform(clean_data).select(\"hashtag_features\", col(\"tweet_id\").alias(\"id2\"))\n",
    "\n",
    "getTime(loading)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Feat_hashtag.write.save(workdir_4e+\"/Feature_hashtag\", format=\"parquet\")\n",
    "Feat_hashtag = spark.read.parquet(workdir_4e+\"/Feature_hashtag\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<p><span style=\"font-size: 13.3333px;\"><strong>Mention Feature Threshold. </strong></span></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mention_tokenizer = Tokenizer(inputCol=\"mention\", outputCol=\"mentions\")\n",
    "mention_cv = CountVectorizer(inputCol=mention_tokenizer.getOutputCol(), outputCol=\"mention_features\", minDF=159)\n",
    "pipeline_mention = Pipeline(stages=[mention_tokenizer,mention_cv])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loading = time.time()\n",
    "\n",
    "model = pipeline_mention.fit(clean_data)\n",
    "Feat_mention = model.transform(clean_data).select(\"mention_features\", col(\"tweet_id\").alias(\"id3\"))\n",
    "\n",
    "getTime(loading)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Feat_mention.write.save(workdir_4e+\"/Feature_mention\", format=\"parquet\")\n",
    "Feat_mention = spark.read.parquet(workdir_4e+\"/Feature_mention\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<p><span style=\"font-size: 13.3333px;\"><strong>User Feature Threshold. </strong></span></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_tokenizer = Tokenizer(inputCol=\"from_user\", outputCol=\"users\")\n",
    "user_cv = CountVectorizer(inputCol=user_tokenizer.getOutputCol(), outputCol=\"user_features\", minDF=159)\n",
    "pipeline_user = Pipeline(stages=[user_tokenizer,user_cv])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loading = time.time()\n",
    "\n",
    "model = pipeline_user.fit(clean_data)\n",
    "Feat_user = model.transform(clean_data).select(\"user_features\", col(\"tweet_id\").alias(\"id4\"))\n",
    "\n",
    "getTime(loading)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Feat_user.write.save(workdir_4e+\"/Feature_user\", format=\"parquet\")\n",
    "Feat_user = spark.read.parquet(workdir_4e+\"/Feature_user\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<p><span style=\"font-size: 13.3333px;\"><strong>Location Feature Threshold. </strong></span></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loc_tokenizer = Tokenizer(inputCol=\"location\", outputCol=\"locs\")\n",
    "loc_cv = CountVectorizer(inputCol=loc_tokenizer.getOutputCol(), outputCol=\"loc_features\", minDF=50)\n",
    "pipeline_loc = Pipeline(stages=[loc_tokenizer,loc_cv])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loading = time.time()\n",
    "\n",
    "model = pipeline_loc.fit(clean_data)\n",
    "Feat_loc = model.transform(clean_data).select(\"loc_features\", \"hashtag\", \"create_time\", col(\"tweet_id\").alias(\"id5\"))\n",
    "\n",
    "getTime(loading)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Feat_loc.write.save(workdir_4e+\"/Feature_loc\", format=\"parquet\")\n",
    "Feat_loc = spark.read.parquet(workdir_4e+\"/Feature_loc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><span style=\"font-size: 13.3333px;\"><strong>Joining all feature DFs above into one. </strong></span></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Feat_1 = Feat_term.join(Feat_hashtag,\\\n",
    "                         Feat_term.tweet_id == Feat_hashtag.id2,\\\n",
    "                         \"inner\").select(Feat_term.term_features,\\\n",
    "                                         Feat_hashtag.hashtag_features,\\\n",
    "                                         Feat_hashtag.id2)\n",
    "Feat_2 = Feat_1.join(Feat_mention,\\\n",
    "                     Feat_1.id2 == Feat_mention.id3,\\\n",
    "                     \"inner\").select(Feat_1.term_features,\\\n",
    "                                     Feat_1.hashtag_features,\\\n",
    "                                     Feat_mention.mention_features,\\\n",
    "                                     Feat_mention.id3)\n",
    "Feat_3 = Feat_2.join(Feat_user,\\\n",
    "                     Feat_2.id3 == Feat_user.id4,\\\n",
    "                     \"inner\").select(Feat_2.term_features,\\\n",
    "                                     Feat_2.hashtag_features,\\\n",
    "                                     Feat_2.mention_features,\\\n",
    "                                     Feat_user.user_features,\\\n",
    "                                     Feat_user.id4)\n",
    "Feat_all = Feat_3.join(Feat_loc,\\\n",
    "                     Feat_3.id4 == Feat_loc.id5,\\\n",
    "                     \"inner\").select(Feat_3.term_features,\\\n",
    "                                     Feat_3.hashtag_features,\\\n",
    "                                     Feat_3.mention_features,\\\n",
    "                                     Feat_3.user_features,\\\n",
    "                                     Feat_loc.loc_features,\\\n",
    "                                     Feat_loc.create_time,\\\n",
    "                                     Feat_loc.hashtag,\\\n",
    "                                     Feat_loc.id5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Feat_all.write.save(workdir_66+\"/Feature_all\", format=\"parquet\")\n",
    "Features_vect = spark.read.parquet(workdir_66+\"/Feature_all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><span style=\"font-size: 13.3333px;\"><strong>Lastly, we join the data frame with the topical labels we had earlier. Now we have both different list of feature vectors and the coresponding labels.</strong></span></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Labled_Feat = Features_vect.join(tennis_labels,\\\n",
    "                                 Features_vect.id5 == tennis_labels.tweet_id,\\\n",
    "                                 \"inner\").select(Features_vect.term_features,\\\n",
    "                                                 Features_vect.hashtag_features,\\\n",
    "                                                 Features_vect.mention_features,\\\n",
    "                                                 Features_vect.user_features,\\\n",
    "                                                 Features_vect.loc_features,\\\n",
    "                                                 Features_vect.create_time,\\\n",
    "                                                 Features_vect.hashtag,\\\n",
    "                                                 tennis_labels.label,\\\n",
    "                                                 tennis_labels.tweet_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Labled_Feat.write.save(workdir_b9+\"/Labled_Features\", format=\"parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Labled_Feat = spark.read.parquet(workdir_b9+\"/Labled_Features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><span style=\"font-size: 13.3333px;\"><strong>At this point, each feature vector is still in its separate column. We need to combine them into one feature matrix. However, before we do that, let's split our dataset first. The reason for this is that Spark is not very good at handling highly sparse data. Saving such data will likely run into memory error. We hold off the combining step for later.</strong></span></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><span style=\"color: #000080;\"><strong>Temporal Split</strong></span></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><span style=\"font-size: 13.3333px;\"><strong>Now we have our feature matrix, it is time to estabulish the training, validation and test set for training the classifier</strong></span></p>\n",
    "<p><span style=\"font-size: 13.3333px;\"><strong>To ensure our classifier generalize to a wide range of features and not simply remeber the past hashtag, we will perform a temporal split to exclude training hashtags in validation and test.</strong></span></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![caption](https://github.com/demoonism/TwitterSensor/blob/master/Screenshot/Capture.JPG?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><span style=\"color: #000080;\"><strong>Hashtag Birthday</strong></span></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><span style=\"font-size: 13.3333px;\"><strong>Hashtag birthday indicates the first timestamp that a particular hashtag appears in the tweet corpus between year 2013 and 2014. We determine this by find the minimum \"create time\" for each hashtag </strong></span></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_birthday = hash_exploded.join(tennis_labels,\\\n",
    "                                 hash_exploded.tweet_id == tennis_labels.tweet_id,\\\n",
    "                                 \"inner\").select(hash_exploded.create_time,\\\n",
    "                                                 hash_exploded.each_hashtag,\\\n",
    "                                                 hash_exploded.tweet_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Find out the \"birthday\", or the earliest appearing time of each hashtag. \n",
    "## (add an extra column of 1 to mark as topical, will be used in a join later)\n",
    "Ordered_Hashtag_set = df_birthday.\\\n",
    "                      groupby(\"each_hashtag\").\\\n",
    "                      agg({\"create_time\": \"min\"}).\\\n",
    "                      orderBy('min(create_time)', ascending=True).\\\n",
    "                      withColumnRenamed(\"min(create_time)\", \"birthday\").\\\n",
    "                      where(df_birthday.each_hashtag.isin(topic_dict[\"Tennis\"])).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Ordered_Hashtag_set.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "time_span = Ordered_Hashtag_set.count()\n",
    "\n",
    "# Get id of the corresponding time split (50% and 60%).\n",
    "train_val_split_Ht = np.floor(np.multiply(time_span, 0.75)).astype(int)\n",
    "val_test_split_Ht =  np.floor(np.multiply(time_span, 0.85)).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Converting to Pandas for random row access.\n",
    "pd_Ordered_Hashtag_set = Ordered_Hashtag_set.toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# locate the timestamp of te 50% and 60% cutoff point. Will be used later to divide D.\n",
    "train_val_time = pd_Ordered_Hashtag_set.iloc[train_val_split_Ht]['birthday']\n",
    "val_test_time = pd_Ordered_Hashtag_set.iloc[val_test_split_Ht]['birthday']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_hashtags = pd_Ordered_Hashtag_set[:train_val_split_Ht][\"each_hashtag\"].tolist()\n",
    "train_hashtags = [x.encode('utf-8') for x in train_hashtags]\n",
    "\n",
    "val_hashtags = pd_Ordered_Hashtag_set[train_val_split_Ht:val_test_split_Ht][\"each_hashtag\"].tolist()\n",
    "val_hashtags = [x.encode('utf-8') for x in val_hashtags]\n",
    "\n",
    "test_hashtags = pd_Ordered_Hashtag_set[val_test_split_Ht:][\"each_hashtag\"].tolist()\n",
    "test_hashtags = [x.encode('utf-8') for x in test_hashtags]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(train_hashtags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(train_val_time)\n",
    "print(val_test_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><span style=\"font-size: 13.3333px;\"><strong>Now we have identified the ids to be used in training, validation and test set, we can proceed to join the id with our feature set to obtain the corresponding data set.</strong></span></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![caption](https://github.com/demoonism/TwitterSensor/blob/master/Screenshot/remove_twit.JPG?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><span style=\"color: #000080;\"><strong>Train-Valid-Test split</strong></span></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Labled_Feat.where(col(\"label\") == 1.0).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Labeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><span style=\"font-size: 13.3333px;\"><strong>All data points happended before the train/valid split time are labeled as training data</strong></span></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Training_set = Labled_Feat.select(Labled_Feat.create_time,\\\n",
    "                              Labled_Feat.tweet_id,\\\n",
    "                              Labled_Feat.term_features,\\\n",
    "                              Labled_Feat.hashtag_features,\\\n",
    "                              Labled_Feat.mention_features,\\\n",
    "                              Labled_Feat.user_features,\\\n",
    "                              Labled_Feat.loc_features,\\\n",
    "                              Labled_Feat.label).where(col(\"create_time\") <= train_val_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loading = time.time()\n",
    "\n",
    "tr_pos_sample = Training_set.where(col(\"label\") == 1.0).count()\n",
    "\n",
    "getTime(loading)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tr_pos_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Labeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><span style=\"font-size: 13.3333px;\"><strong>All data points happended between the train/valid & valid/test split time are labeled as validation data. We also need to remove any hashtag that appeared in the training set.</strong></span></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Validation_set = Labled_Feat.select(Labled_Feat.create_time,\\\n",
    "                                Labled_Feat.tweet_id,\\\n",
    "                                Labled_Feat.term_features,\\\n",
    "                                Labled_Feat.hashtag_features,\\\n",
    "                                Labled_Feat.mention_features,\\\n",
    "                                Labled_Feat.user_features,\\\n",
    "                                Labled_Feat.loc_features,\\\n",
    "                                Labled_Feat.hashtag,\\\n",
    "                                Labled_Feat.label).where((col(\"create_time\") > train_val_time) & (col(\"create_time\") <= val_test_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hashtag_tokenizer = Tokenizer(inputCol=\"hashtag\", outputCol=\"each_hashtag\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val_hashtags_df = hashtag_tokenizer.transform(Validation_set)\n",
    "hashtag =  val_hashtags_df.select(\"tweet_id\",\"each_hashtag\")\n",
    "val_hash_exploded = hashtag.withColumn('each_hashtag', explode('each_hashtag'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#val_hash_exploded.write.save(workdir_66+\"/validation_hash_exploded\", format=\"parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val_hash_exploded = spark.read.parquet(workdir_66+\"/validation_hash_exploded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Invalid_Val_ids = val_hash_exploded.select(\"tweet_id\").\\\n",
    "                                           where(val_hash_exploded.each_hashtag.isin(train_hashtags)).\\\n",
    "                                           distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Invalid_Val_ids_list = Invalid_Val_ids.distinct().rdd.flatMap(lambda x: x).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## If a hashtag appeared in training set, discard this record\n",
    "\n",
    "Validation_set_no_train = Validation_set.where(Validation_set.tweet_id.isin(Invalid_Val_ids_list) == False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val_pos_sample = Validation_set_no_train.where(col(\"label\") == 1.0).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val_pos_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Validation_set_no_train.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Labeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><span style=\"font-size: 13.3333px;\"><strong>All data points happended after the valid/test split time are labeled as test data. We also need to remove any hashtag that appeared in training&validation set.</strong></span></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Test_set = Labled_Feat.select(Labled_Feat.create_time,\\\n",
    "                            Labled_Feat.tweet_id,\\\n",
    "                            Labled_Feat.term_features,\\\n",
    "                            Labled_Feat.hashtag_features,\\\n",
    "                            Labled_Feat.mention_features,\\\n",
    "                            Labled_Feat.user_features,\\\n",
    "                            Labled_Feat.loc_features,\\\n",
    "                            Labled_Feat.hashtag,\\  \n",
    "                            Labled_Feat.label).where(col(\"create_time\") > val_test_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_hashtags_df = hashtag_tokenizer.transform(Test_set)\n",
    "hashtag = test_hashtags_df.select(\"tweet_id\",\"each_hashtag\")\n",
    "test_hash_exploded = hashtag.withColumn('each_hashtag', explode('each_hashtag'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_hash_exploded.write.save(workdir_66+\"/test_hash_exploded_beta1\", format=\"parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_hash_exploded = spark.read.parquet(workdir_66+\"/test_hash_exploded_beta1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Invalid_Test_ids = test_hash_exploded.select(\"tweet_id\").where((test_hash_exploded.each_hashtag.isin(train_hashtags)) | (test_hash_exploded.each_hashtag.isin(val_hashtags))).distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Invalid_Test_ids_list = Invalid_Test_ids.distinct().rdd.flatMap(lambda x: x).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Test_set_no_train_no_vaild = Test_set.where(Test_set.tweet_id.isin(Invalid_Test_ids_list) == False).\\\n",
    "                                         dropDuplicates(['from_user', 'hashtag', 'location', 'mention', 'term'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling data to balance label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><span style=\"font-size: 13.3333px;\"><strong> Downsampling negative data to balance out the training data</strong></span></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Concatenate pos and neg training samples to form the final training set.\n",
    "\n",
    "Training_set_balanced = Training_set.sampleBy(\"label\", fractions={0.0: 0.001, 1.0: 1}, seed=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Training_set_balanced.write.save(workdir_b9+\"/Train_balanced\", format=\"parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Training_set_balanced = spark.read.parquet(workdir_b9+\"/Train_balanced\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Validation_set_no_train_balanced = Validation_set_no_train..sampleBy(\"label\", fractions={0.0: 0.001, 1.0: 1}, seed=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Validation_set_no_train_balanced.write.save(workdir_b9+\"/Valid_balanced\", format=\"parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Validation_set_no_train_balanced = spark.read.parquet(workdir_b9+\"/Valid_balanced\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><span style=\"font-size: 13.3333px;\"><strong>Now we have a much smaller dataset to work with, it is time to go ahead and concatenate the feature vectors to obtain a single feature matrix. Note that we still keep the tweet id in the output because we want to keep a mapping to the original tweet for manual examination</strong></span></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Assembling(ds):\n",
    "    assembler = VectorAssembler(inputCols = [\"term_features\",\"hashtag_features\",\"mention_features\",\"user_features\",\"loc_features\"], outputCol=\"features\")\n",
    "    assembled_dataset = assembler.transform(ds).\\\n",
    "                    select(\"tweet_id\",\"create_time\",\"features\", \"label\")\n",
    "    return assembled_dataset\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Tr_Features = Assembling(Training_set_balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Val_Features =  Assembling(Validation_set_no_train_balanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><span style=\"font-size: 13.3333px;\"><strong>We also get the text for each feature by printing out the vocabular. This is useful for manual inspection.</strong></span></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Input = dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#dataset.write.save(workdir_b9+\"/Labled_Feature_Assembled_bal\", format=\"parquet\")\n",
    "#dataset.write.json(workdir_b9+\"/Labled_Feature_Assembled_bal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Training_set_balanced = spark.read.parquet(workdir_b9+\"/Labled_Feature_Assembled_bal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Training_set_balanced.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import SparseVector, VectorUDT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load json object, if a line is invalid, substitute as an empty dict (which has len() == 0 )\n",
    "\n",
    "def loadSparseVector(d):\n",
    "    try:\n",
    "        js = json.loads(d)\n",
    "        feat = SparseVector(js['features']['size'], js['features']['indices'], js['features']['values'])\n",
    "        updated = {'create_time': js['create_time'],\n",
    "        'features': feat,                  \n",
    "        'tweet_id': js['tweet_id'],\n",
    "        'label': js['label']}        \n",
    "    except ValueError as e:\n",
    "        updated = {}\n",
    "    except Exception:\n",
    "        updated = {}\n",
    "    return updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = sc.textFile(workdir_b9+\"/Labled_Feature_Assembled_Json/*.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = dataset.map(loadSparseVector)\n",
    "# Taking a look at the (parsed) first line of our input files\n",
    "data.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Define Dataframe schema.\n",
    "Feat_schema = StructType([StructField('create_time', DoubleType(), False),\n",
    "                     StructField('features', VectorUDT(), False),\n",
    "                     StructField('label', DoubleType(), False),\n",
    "                     StructField('tweet_id', StringType(), False)\n",
    "                    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = sqlContext.createDataFrame(data, Feat_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><span style=\"color: #000080;\"><strong><span style=\"font-family: georgia, palatino, serif; font-size: 24pt;\">Step Three: Training Classifier</span></strong></span></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><span style=\"font-size: 13.3333px;\"><strong>To train an effective classifier, we need to follow two steps: 1. Feature selection 2. Parameter tunning. Here we will be using Chi-sqaure as our feature selection method and tunning L2 penalty and epoch accordingly.</strong></span></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><span style=\"font-size: 13.3333px;\"><strong>Wrap feature selector into a pipeline, use grid search to determine the optimal number of features. Chi-Square is a similar feature selection technique as mutual information. Utilizing this as its spark-built-in.</strong></span></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import ChiSqSelector\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "selector = ChiSqSelector(featuresCol=\"features\",\n",
    "                         outputCol=\"Features_matrix\", labelCol=\"label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><span style=\"color: #000080;\"><strong>Train logistic regression and Hyper Parameter Tunning</strong></span></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><span style=\"font-size: 13.3333px;\"><strong>We are tunning two hyperparameters for the logistic regression, namly number of features and L2 penalty</strong></span></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "from LogLossEvaluator import BinaryRankingEvaluator\n",
    "\n",
    "blor = LogisticRegression(featuresCol='Features_matrix', labelCol='label')\n",
    "\n",
    "TrainingPipeline = Pipeline(stages=[selector,blor])\n",
    "\n",
    "Ranker = BinaryRankingEvaluator(metric = \"AP\")\n",
    "\n",
    "# Run cross-validation, and choose the best set of parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><span style=\"font-size: 13.3333px;\"><strong>Extending the crossValidator class to use our custom  evaluator (P@k and MAP). The Evaluator code can be found in LogLossEvaluator.py</strong></span></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from pyspark.ml.tuning import CrossValidator, CrossValidatorModel, ParamGridBuilder\n",
    "from pyspark.sql.functions import rand\n",
    "\n",
    "from pyspark.sql import Row\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from LogLossEvaluator import BinaryRankingEvaluator\n",
    "\n",
    "result = []\n",
    "Ranker = BinaryRankingEvaluator(metric = \"AP\")\n",
    "\n",
    "class CrossValidatorVerbose(CrossValidator):\n",
    "\n",
    "    def _fit(self, dataset):\n",
    "        est = self.getOrDefault(self.estimator)\n",
    "        epm = self.getOrDefault(self.estimatorParamMaps)\n",
    "        numModels = len(epm)\n",
    "        print(\"total of\", numModels, \" models\")\n",
    "              \n",
    "        eva = self.getOrDefault(self.evaluator)\n",
    "        #metricName = eva.getMetricName()\n",
    "        metricName = \"AP\"\n",
    "        #print(\"metric is \", metricName)\n",
    "              \n",
    "        nFolds = self.getOrDefault(self.numFolds)\n",
    "        seed = self.getOrDefault(self.seed)\n",
    "        metrics = [0.0] * numModels\n",
    "\n",
    "\n",
    "        print(\"valid size \",dataset.count())\n",
    "        train = dataset\n",
    "        print(\"train size \", dataset.count())\n",
    "\n",
    "        for j in range(numModels):\n",
    "            paramMap = epm[j]\n",
    "            model = est.fit(train, paramMap)\n",
    "\n",
    "            predictions = model.transform(dataset, paramMap)\n",
    "            #print(predictions.show())\n",
    "            metric = eva.evaluate(predictions)\n",
    "            metrics[j] += metric\n",
    "\n",
    "            avgSoFar = metrics[j] / foldNum\n",
    "\n",
    "            res=(\"params: %s\\t%s: %f\\tavg: %f\" % (\n",
    "                {param.name: val for (param, val) in paramMap.items()},\n",
    "                metricName, metric, avgSoFar))\n",
    "            result.append(res)\n",
    "            print(res)\n",
    "\n",
    "        if eva.isLargerBetter():\n",
    "            bestIndex = np.argmax(metrics)\n",
    "        else:\n",
    "            bestIndex = np.argmin(metrics)\n",
    "\n",
    "        bestParams = epm[bestIndex]\n",
    "        bestModel = est.fit(dataset, bestParams)\n",
    "        avgMetrics = [m / nFolds for m in metrics]\n",
    "        bestAvg = avgMetrics[bestIndex]\n",
    "        print(\"Best model:\\nparams: %s\\t%s: %f\" % (\n",
    "            {param.name: val for (param, val) in bestParams.items()},\n",
    "            metricName, bestAvg))\n",
    "\n",
    "        return self._copyValues(CrossValidatorModel(bestModel, avgMetrics))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "paramGrid = ParamGridBuilder().\\\n",
    "    addGrid(selector.numTopFeatures, [10, 100, 1000, 10000, 100000, 1000000000]).\\ # number of feature to use, last one = all features.\n",
    "    addGrid(blor.regParam, [0.0001, 0.001, 0.01, 0.1, 1, 10]).\\\n",
    "    addGrid(blor.maxIter, [5, 10, 50,100]).\\\n",
    "    build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cvImplicit = CrossValidatorVerbose(estimator=TrainingPipeline, numFolds=1, estimatorParamMaps=paramGrid,evaluator=Ranker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cvModel = cvImplicit.fit(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(cvModel.avgMetrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for a,b in zip(paramGrid, cvModel.avgMetrics):\n",
    "    print(b,a)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cvModel.bestModel.getParam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pr = cvModel.bestModel.transform(Validation_set_no_train)\n",
    "metric = Ranker.evaluate(pr)\n",
    "print(\"Best Validation AP: \", metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pr = cvModel.bestModel.transform(Test_set_no_train_no_vaild)\n",
    "metric = Ranker.evaluate(pr)\n",
    "print(\"Test AP: \", metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Ranked_prediction.select(\"term\",\"hashtag\",\"from_user\").show(100, truncate = False)\n",
    "Test_set_no_train_no_vaild.select(\"hashtag\").where(col(\"label\") == 1.0).show(100, truncate =  False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
