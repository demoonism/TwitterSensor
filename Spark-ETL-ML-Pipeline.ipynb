{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><span style=\"font-size: 36pt; font-family: georgia, palatino, serif; color: #800000;\">Learning Topical Social Sensors</span></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><span style=\"color: #000080;\"><strong>How useful is twitter to you in terms of finding the right information?</strong></span></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![caption](https://github.com/demoonism/TwitterSensor/blob/master/Screenshot/search.JPG?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: center;\"><span style=\"text-decoration: underline;\"><span style=\"font-size: 20pt;\"><em><strong>We can do better than this!</strong> </em></span></span></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: left;\"><strong>In this project, we are aiming to train a classifier to identify targeted information on Twitter with high precision. </strong></p>\n",
    "<p style=\"text-align: left;\"><strong>For example, if you are interested in:</strong></p>\n",
    "<p style=\"text-align: left;\"><em><strong>&bull; Global social issues</strong></em><br /><em><strong>&bull; Politics in the Pacific Northwest</strong></em><br /><em><strong>&bull; Public transit in New York City</strong></em></p>\n",
    "<p style=\"text-align: left;\"><strong>The classifier would serve as a \"sensor\" to identify topical tweets based on your tailored interests!</strong></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><span style=\"color: #000080;\"><strong>Challenges</strong></span></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: left;\"><strong>(1) &nbsp;Billions of potential features, thousands of useful ones (Hashtags, users, mentions, terms, locations)</strong></p>\n",
    "<p style=\"text-align: left;\"><strong>(2) &nbsp;Need a lot of labeled data to learn feature weights well</strong></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><span style=\"color: #000080;\"><strong>Solution</strong></span></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><span style=\"font-size: 12pt;\"><strong>(1) Careful feature engineering and feature selection using Apache Spark.</strong></span></p>\n",
    "<p><span style=\"font-size: 10pt;\"><strong>We performed feature selection and transformation with Apache Spark on a standalone server with eight 1TB Hard disks, two 20 core CPU (40 threads) and 256GB RAM. </strong></span></p>\n",
    "<p><span style=\"font-size: 12pt;\"><strong>(2)</strong> <strong>Hashtags!</strong>&nbsp;</span></p>\n",
    "<p><span style=\"font-size: 10pt;\"><strong>Hashtags&nbsp;originated on IRC chat, were&nbsp;adopted later (and perhaps most famously) on Twitter, and&nbsp;now appear on other social media platforms such as Instagram,&nbsp;Tumblr, and Facebook. They usually serve as surogates for topics. Therefore, for each topic,&nbsp;we leverage a (small)&nbsp;set of user-curated topical hashtags to efficiently provide&nbsp;a large number of supervised topic labels for social media&nbsp;content.&nbsp;</strong></span></p>\n",
    "<p><span style=\"font-size: 10pt;\"><strong>We used 4 independent annotators to query the Twitter search API to identify candidate hashtags for each topic. A&nbsp;hashtag is assigned to a topic set if 3 out of 4 annotators agrees on the assignment.</strong></span></p>\n",
    "<p><span style=\"font-size: 10pt;\"><strong>For example, for the topic, \"Natural Disaster\", the set of hashtags are [\"sandy\", \"drought\", \"storm\", \"hurricane\", \"tornado\" .... etc]. If a tweet contains one or more of the pre-determined hashtags, we say it is \"topical\" for a particular toic, and it is labeled 1 (0 otherwise). We will revisit this in the feature selection section</strong></span></p>\n",
    "<p><span style=\"font-size: 10pt;\">&nbsp;</span></p>\n",
    "<p><span style=\"font-size: 18pt; color: #ff0000;\"><strong>Catch!</strong></span></p>\n",
    "<p><strong><span style=\"font-size: 10pt;\">Hashtag is part of our feature, wouldn't the classifier simply learn to remember the hashtag?</span></strong></p>\n",
    "<p><strong><span style=\"font-size: 10pt;\">To ensure maximum generality, we remove training hashtags from the validation and test set to ensure the classifier making prediction on the learnt feature and not just remembering hashtags. This would be further illlustrated in the Train-Validation split section later.</span></strong></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><span style=\"color: #000080;\"><strong>Now we have labeled data, what features could be useful for predciting topicality?</strong></span></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![caption](https://github.com/demoonism/TwitterSensor/blob/master/Screenshot/twt.JPG?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: left;\"><span style=\"font-size: 18pt; color: #000080;\"><strong>Why might these tweet features be useful?</strong> </span></p>\n",
    "<p style=\"text-align: left;\"><br /><span style=\"font-size: 10pt;\"><strong>&bull; Users: who tweets on the topic?</strong></span><br /><span style=\"font-size: 10pt;\"> <em><strong>-</strong></em> <span style=\"text-decoration: underline;\"><em><strong>Tweets from the weather channel might be a good indicator for Natural Disasters</strong></em></span></span></p>\n",
    "<p style=\"text-align: left;\"><br /><span style=\"font-size: 10pt;\"><strong>&bull; Hashtags: What hashtags co-occur with the topic?</strong></span><br /><span style=\"font-size: 10pt;\"> <em><strong>-</strong></em> <span style=\"text-decoration: underline;\"><em><strong>#teaparty could imply LBGT rights</strong></em></span></span></p>\n",
    "<p style=\"text-align: left;\"><br /><span style=\"font-size: 10pt;\"><strong>&bull; Mentions:</strong></span><br /><span style=\"font-size: 10pt;\"> <em><strong>-</strong></em> <span style=\"text-decoration: underline;\"><em><strong>@Redcross might be releavant to Natural Disaster</strong></em></span></span></p>\n",
    "<p style=\"text-align: left;\"><br /><span style=\"font-size: 10pt;\"><strong>&bull; Locations:</strong></span><br /><span style=\"font-size: 10pt;\"> <em><strong>-</strong></em> <span style=\"text-decoration: underline;\"><em><strong>Philippines where a lot of natural disaster happend in the last few years is a descent guess for releavant topics</strong></em></span></span></p>\n",
    "<p style=\"text-align: left;\"><br /><span style=\"font-size: 10pt;\"><strong>&bull; Terms:</strong></span><br /><span style=\"font-size: 10pt;\"> <em><strong>-</strong></em> <span style=\"text-decoration: underline;\"><em><strong> Word features are strong indicators of a particular topic</strong></em></span></span></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><span style=\"color: #000080;\"><strong>Implementation</strong></span></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><strong><span style=\"font-size: 10pt;\">The original Twitter data were collected over 2 years, which contains over 2TB compressed data. It consists of hundreds of millions lines of tweets.</span></strong></p>\n",
    "<p><strong><span style=\"font-size: 10pt;\">How do we go from the raw data to an efficient classifier?</span></strong></p>\n",
    "<p><strong><span style=\"font-size: 10pt;\">The following three-step processes serves an end-to-end pipeline to perform ETL and ML training.</span></strong></p>\n",
    "<p>&nbsp;</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><span style=\"color: #000080;\"><strong><span style=\"font-family: georgia, palatino, serif; font-size: 24pt;\">Step One: Pre-Processing</span></strong></span></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><span style=\"font-size: 13.3333px;\"><strong>Each valid tweet crawled from the server is a json object with over 100 attributes. An example could be find as following:</strong></span></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><span style=\"font-size: 10pt;\"><strong>Sample Tweet</strong></span></p>\n",
    "<p><span style=\"font-size: 8pt;\"><strong>{</strong>\"created_at\":\"Thu Jan 31 12:58:06 +0000 2013\",</span><br /><span style=\"font-size: 8pt;\"> \"id\":296965581582786560,</span><br /><span style=\"font-size: 8pt;\"> \"id_str\":\"296965581582786560\",</span><br /><span style=\"font-size: 8pt;\"> \"text\":\"Im ready for whatever\",</span><br /><span style=\"font-size: 8pt;\"> \"source\":\"\\u003ca href=\\\"http:\\/\\/twitter.com\\/download\\/iphone\\\" rel=\\\"nofollow\\\"\\u003eTwitter for iPhone\\u003c\\/a\\u003e\",</span><br /><span style=\"font-size: 8pt;\"> \"truncated\":false,</span><br /><span style=\"font-size: 8pt;\"> \"in_reply_to_status_id\":null,</span><br /><span style=\"font-size: 8pt;\"> \"in_reply_to_status_id_str\":null,</span><br /><span style=\"font-size: 8pt;\"> \"in_reply_to_user_id\":null,</span><br /><span style=\"font-size: 8pt;\"> \"in_reply_to_user_id_str\":null,</span><br /><span style=\"font-size: 8pt;\"> \"in_reply_to_screen_name\":null,</span><br /><span style=\"font-size: 8pt;\"> \"user\":{</span><br /><span style=\"font-size: 8pt;\"> \"id\":1059349532,</span><br /><span style=\"font-size: 8pt;\"> \"id_str\":\"1059349532\",</span><br /><span style=\"font-size: 8pt;\"> \"name\":\"Don Dada\",</span><br /><span style=\"font-size: 8pt;\"> \"screen_name\":\"ImDatNiggaBD\",</span><br /><span style=\"font-size: 8pt;\"> \"location\":\"South Side Of Little Rock\",</span><br /><span style=\"font-size: 8pt;\"> \"url\":null,</span><br /><span style=\"font-size: 8pt;\"> \"description\":\"Weed Smoker (Kush)\",</span><br /><span style=\"font-size: 8pt;\"> \"protected\":false,</span><br /><span style=\"font-size: 8pt;\"> \"followers_count\":109,</span><br /><span style=\"font-size: 8pt;\"> \"friends_count\":110,</span><br /><span style=\"font-size: 8pt;\"> \"listed_count\":0,</span><br /><span style=\"font-size: 8pt;\"> \"created_at\":\"Fri Jan 04 02:37:28 +0000 2013\",</span><br /><span style=\"font-size: 8pt;\"> \"favourites_count\":14,</span><br /><span style=\"font-size: 8pt;\"> \"utc_offset\":null,</span><br /><span style=\"font-size: 8pt;\"> \"time_zone\":null,</span><br /><span style=\"font-size: 8pt;\"> \"geo_enabled\":false,</span><br /><span style=\"font-size: 8pt;\"> \"verified\":false,</span><br /><span style=\"font-size: 8pt;\"> \"statuses_count\":1312,</span><br /><span style=\"font-size: 8pt;\"> \"lang\":\"en\",</span><br /><span style=\"font-size: 8pt;\"> \"contributors_enabled\":false,</span><br /><span style=\"font-size: 8pt;\"> \"is_translator\":false,</span><br /><span style=\"font-size: 8pt;\"> \"profile_background_color\":\"C0DEED\",</span><br /><span style=\"font-size: 8pt;\"> \"profile_background_image_url\":\"http:\\/\\/a0.twimg.com\\/images\\/themes\\/theme1\\/bg.png\",</span><br /><span style=\"font-size: 8pt;\"> \"profile_background_image_url_https\":\"https:\\/\\/si0.twimg.com\\/images\\/themes\\/theme1\\/bg.png\",</span><br /><span style=\"font-size: 8pt;\"> \"profile_background_tile\":false,</span><br /><span style=\"font-size: 8pt;\"> \"profile_image_url\":\"http:\\/\\/a0.twimg.com\\/profile_images\\/3184813228\\/d6d3a95d902f088f412cf1bd90c126c7_normal.jpeg\",</span><br /><span style=\"font-size: 8pt;\"> \"profile_image_url_https\":\"https:\\/\\/si0.twimg.com\\/profile_images\\/3184813228\\/d6d3a95d902f088f412cf1bd90c126c7_normal.jpeg\",</span><br /><span style=\"font-size: 8pt;\"> \"profile_banner_url\":\"https:\\/\\/si0.twimg.com\\/profile_banners\\/1059349532\\/1359068332\",</span><br /><span style=\"font-size: 8pt;\"> \"profile_link_color\":\"0084B4\",</span><br /><span style=\"font-size: 8pt;\"> \"profile_sidebar_border_color\":\"C0DEED\",</span><br /><span style=\"font-size: 8pt;\"> \"profile_sidebar_fill_color\":\"DDEEF6\",</span><br /><span style=\"font-size: 8pt;\"> \"profile_text_color\":\"333333\",</span><br /><span style=\"font-size: 8pt;\"> \"profile_use_background_image\":true,</span><br /><span style=\"font-size: 8pt;\"> \"default_profile\":true,</span><br /><span style=\"font-size: 8pt;\"> \"default_profile_image\":false,</span><br /><span style=\"font-size: 8pt;\"> \"following\":null,</span><br /><span style=\"font-size: 8pt;\"> \"follow_request_sent\":null,</span><br /><span style=\"font-size: 8pt;\"> \"notifications\":null},</span><br /><span style=\"font-size: 8pt;\"> \"geo\":null,</span><br /><span style=\"font-size: 8pt;\"> \"coordinates\":null,</span><br /><span style=\"font-size: 8pt;\"> \"place\":null,</span><br /><span style=\"font-size: 8pt;\"> \"contributors\":null,</span><br /><span style=\"font-size: 8pt;\"> \"retweet_count\":0,</span><br /><span style=\"font-size: 8pt;\"> \"entities\":{\"hashtags\":[],</span><br /><span style=\"font-size: 8pt;\"> \"urls\":[],</span><br /><span style=\"font-size: 8pt;\"> \"user_mentions\":[]},</span><br /><span style=\"font-size: 8pt;\"> \"favorited\":false,</span><br /><span style=\"font-size: 8pt;\"> \"retweeted\":false,</span><br /><span style=\"font-size: 8pt;\"> \"lang\":\"en\"<strong>}</strong></span></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><span style=\"font-size: 13.3333px;\"><strong>Obviously, not all attributes are relevant to our analysis. In the context of this paper, the only releavant fields in our features are:</strong></span></p>\n",
    "<p><span style=\"color: #0000ff;\"><em><strong>Hashtags, From_User, Create_Time, Location, Mentions</strong></em></span></p>\n",
    "<p><span style=\"font-size: 13.3333px;\"><strong>Moreover, the raw text is quite dirty. We need to perform some data cleaning in order to get clean features.</strong></span></p>\n",
    "<p><span style=\"font-size: 13.3333px;\"><strong>Since is step is fairly involveda and independet of the analysis here, I keep them in a separate Notebook. </strong></span></p>\n",
    "<blockquote>\n",
    "<p><span style=\"font-size: 13.3333px;\"><strong>Spark-Twt-PreProcessing.ipynb</strong></span></p>\n",
    "</blockquote>\n",
    "<p><span style=\"font-size: 13.3333px;\"><strong> You should be able to follow along as an indepent module.</strong></span></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><span style=\"font-size: 13.3333px;\"><strong>The resulting data looks like this:</strong></span></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><strong>Processed-tweet:</strong></p>\n",
    "<p><strong>{</strong>u'Create_time': 1359737884.0,<br /> u'from_id': 87151732,<br /> u'from_user': u'ishiPTI',<br /> u'hashtag': u'thuglife',<br /> u'location': u'loc_lakeshore',<br /> u'mention': u'BushraShekhani',<br /> u'term': u'I am ready for whatever',<br /> u'tweet_id': 297312861586325504<strong>}</strong></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><span style=\"font-size: 13.3333px;\"><strong>Now we have a small (sort of) and clean dataset to work with, it is time to move on to spark to perform some reall analysis.</strong></span></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><span style=\"color: #000080;\"><strong><span style=\"font-family: georgia, palatino, serif; font-size: 24pt;\">Step Two: Feature Extraction</span></strong></span></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><span style=\"font-size: 13.3333px;\"><strong>We need to turn the raw json data into a feature matrix. There are two keys here: </strong></span></p>\n",
    "<p><span style=\"font-size: 13.3333px;\"><strong>1. Data processing must be extremly efficient since we only have 40 cores and 256G ram.</strong></span></p>\n",
    "<p><span style=\"font-size: 13.3333px;\"><strong>2. The resulting matrix must be sparse to facilitate the training step&nbsp;later.</strong></span></p>\n",
    "<p><span style=\"font-size: 13.3333px;\"><strong>These are achieved through the following pipeline. </strong></span></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Notebook property setup.\n",
    "## Spark SQL\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import udf, col, lit, monotonically_increasing_id, explode\n",
    "from pyspark.sql import functions as F\n",
    "## Spark ML\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.ml.classification import LogisticRegression, DecisionTreeClassifier\n",
    "from pyspark.ml.param import Param, Params\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer, IDF, StopWordsRemover, CountVectorizer, VectorAssembler\n",
    "\n",
    "## Helper\n",
    "import sys\n",
    "import time\n",
    "import os.path\n",
    "import json\n",
    "from datetime import datetime\n",
    "from operator import add\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "## Enable inline graphs\n",
    "%matplotlib inline\n",
    "\n",
    "import preprocessor as p\n",
    "import string\n",
    "\n",
    "## Display precision for pandas dataframe\n",
    "pd.set_option('precision',10)\n",
    "\n",
    "## Helper function to keep track the run time of a spark ops.\n",
    "def getTime(start):\n",
    "    sec = time.time() - start\n",
    "    m, s = divmod(sec, 60)\n",
    "    h, m = divmod(m, 60)\n",
    "    print('Spark operation takes - %d:%02d:%02d which is %d seconds in total' % (h,m,s,sec))\n",
    "    \n",
    "# load json object, if a line is invalid, substitute as an empty dict (which has len() == 0 )\n",
    "def loadJson(d):\n",
    "    try:\n",
    "        js = json.loads(d)\n",
    "    except ValueError as e:\n",
    "        js = {}\n",
    "    except Exception:\n",
    "        js = {}\n",
    "    return js\n",
    "\n",
    "def translating(x):\n",
    "    return x.encode('utf-8').lower().translate(None, string.punctuation)\n",
    "\n",
    "def Cleansing(d):\n",
    "    txt = p.clean(d['term'].encode('ascii', 'ignore')).replace(\":\", \"\").lower()\n",
    "\n",
    "    if d['location'] == None:\n",
    "        loc_term = \"empty_location\"\n",
    "    elif d['location'].strip(' ') == '':\n",
    "        loc_term = \"empty_location\"\n",
    "    else:\n",
    "        loc_term = 'loc_' + \"_\".join(map(translating, d['location'].strip(' ').split(\" \")))\n",
    "        \n",
    "    if txt == None:\n",
    "        terms = \"empty_tweet\"\n",
    "    elif txt.strip(' ') == '':\n",
    "        terms = \"empty_tweet\"\n",
    "    else:\n",
    "        terms = txt.encode('utf-8').translate(None, string.punctuation).strip(' ')\n",
    "        \n",
    "    if d['hashtag'] == None:\n",
    "        hashtags = \"empty_hashtag\"\n",
    "    elif d['hashtag'].strip(' ') == '':\n",
    "        hashtags = \"empty_hashtag\"\n",
    "    else:\n",
    "        hashtags = d['hashtag']\n",
    "        \n",
    "    if d['mention'] == None:\n",
    "        mentions = \"empty_mention\"\n",
    "    elif d['mention'].strip(' ') == '':\n",
    "        mentions = \"empty_mention\"\n",
    "    else:\n",
    "        mentions = d['mention']\n",
    "        \n",
    "    processed = {\"from_user\":d['from_user'],\n",
    "                 \"from_id\":d['from_id'],\n",
    "                 \"tweet_id\":d['tweet_id'],\n",
    "                 \"hashtag\":hashtags,\n",
    "                 \"term\": terms,\n",
    "                 \"location\":loc_term,\n",
    "                 \"mention\":mentions,\n",
    "                 \"create_time\":d['create_time']\n",
    "                }\n",
    "    return processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><span style=\"font-size: 18px;\"><strong>Reading DATA </strong></span></p>\n",
    "<p><span style=\"font-size: 13.3333px;\"><strong>After preprocessing, data are saved as parquet fiels. We need to load and parse these data into dataframes. Note that, the sc.textFile function's input directory could be either a file or a directory. Spark context will create partitions automatically. Note that the pre-processed data are stored in two directories.</strong></span></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# full\n",
    "data_Eng = sc.textFile(\"/mnt/1e69d2b1-91a9-473c-a164-db90daf43a3d/Eng_Json/,/mnt/2b53fde0-61da-4eeb-a038-9910540ff9ad/Eng_Json/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><span style=\"font-size: 13.3333px;\"><strong>Taking a look at the (parsed) first line of our input files </strong></span></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{u'create_time': 1380034531.0,\n",
       "  u'from_id': u'330743066',\n",
       "  u'from_user': u'maaddieeeb',\n",
       "  u'hashtag': u'',\n",
       "  u'location': u'',\n",
       "  u'mention': u'RickyPDillon tyleroakley',\n",
       "  u'term': u'RT @RickyPDillon: @tyleroakley you literally travel the world can i have your life',\n",
       "  u'tweet_id': u'382458268779421696'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data_Eng.map(loadJson)\n",
    "data.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><span style=\"color: #000080;\"><strong>Turning to dataframe</strong></span></h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><span style=\"font-size: 13.3333px;\"><strong>An RDD (Resilient Distributed Dataset) is more of a blackbox dataset that cannot be easily optimized as the operations that can be performed against it are not as constrained. (Available in Spark since 1.0) </strong></span></p>\n",
    "<p><span style=\"font-size: 13.3333px;\"><strong>A dataframe is a table, or two-dimensional array-like structure, in which each column contains measurements on one variable, and each row contains one case. Therefore, a DataFrame has additional metadata due to its tabular format, which allows Spark to run certain optimizations on the finalized query. (Added since 1.3) </strong></span></p>\n",
    "<p><span style=\"font-size: 13.3333px;\"><strong>In summary, you are able to write traditional map-reduce type of code on both RDD and Dataframe, but Dataframe also support SQL command and built-in analytical functions. For performance conideration, we are turning our RDD into Dataframes first.</strong></span></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Define Dataframe schema.\n",
    "schema = StructType([StructField('create_time', DoubleType(), False),\n",
    "                     StructField('from_id', StringType(), False),\n",
    "                     StructField('from_user', StringType(), False),\n",
    "                     StructField('hashtag', StringType(), True),\n",
    "                     StructField('location', StringType(), True),\n",
    "                     StructField('mention', StringType(), True),\n",
    "                     StructField('term', StringType(), True),\n",
    "                     StructField('tweet_id', StringType(), False)\n",
    "                    ])\n",
    "df = sqlContext.createDataFrame(data, schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><span style=\"font-size: 13.3333px;\"><strong>Input are shown as tabular form (Dataframe) below. Note that hashtag field is happend to be null for the first few records. </strong></span></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------------+-------+--------------------+--------------------+--------------------+------------------+\n",
      "|  create_time|   from_id|   from_user|hashtag|            location|             mention|                term|          tweet_id|\n",
      "+-------------+----------+------------+-------+--------------------+--------------------+--------------------+------------------+\n",
      "|1.380034531E9| 330743066|  maaddieeeb|       |                    |RickyPDillon tyle...|RT @RickyPDillon:...|382458268779421696|\n",
      "|1.380034532E9| 993373555|deanojames95|       |          Birmingham|spinky1996 deanoj...|RT @spinky1996: @...|382458272940183552|\n",
      "|1.380034532E9|1378264339|  JonahsEyes|       |3/6 closet drake ...|                    |WHY AM I SITTING ...|382458272940589057|\n",
      "|1.380034532E9| 603247328|emilybalzano|       |                    |                    |3 people followed...|382458272974135296|\n",
      "|1.380034532E9|1336196430|Bah7ar_Fa7al|       |             Bahrain|            TellyApp|I like 'Check out...|382458272978329600|\n",
      "+-------------+----------+------------+-------+--------------------+--------------------+--------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23937570"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(\"location\").distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><span style=\"font-size: 13.3333px;\"><strong>Now we need to transform the textual features into a sparse vector to be piped into our learning algorithm later.</strong></span></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><span style=\"color: #000080;\"><strong>Vectorizing user, hashtag, location, mention, term into feature vectors</strong></span></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><span style=\"font-size: 13.3333px;\"><strong>We use the same threshold as describbed in the paper. Note that the threshold is for DF, not TF.</strong></span></p>\n",
    "<table style=\"height: 51px; margin-left: auto; margin-right: auto;\" width=\"30\">\n",
    "<tbody>\n",
    "<tr>\n",
    "<td><strong>Feature</strong></td>\n",
    "<td><strong>Threshold</strong></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>From_User</td>\n",
    "<td>159</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Hashtag</td>\n",
    "<td>159</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Mention</td>\n",
    "<td>159</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Location</td>\n",
    "<td>50</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>term</td>\n",
    "<td>50</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><span style=\"font-size: 13.3333px;\"><strong>In this section, we vectorize each feature according to the count threshold above. </strong></span></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "term_tokenizer = Tokenizer(inputCol=\"term\", outputCol=\"words\")\n",
    "term_remover = StopWordsRemover(inputCol=term_tokenizer.getOutputCol(), outputCol=\"filtered\")\n",
    "term_cv = CountVectorizer(inputCol=term_remover.getOutputCol(), outputCol=\"term_features\", minDF=50)\n",
    "\n",
    "hashtag_tokenizer = Tokenizer(inputCol=\"hashtag\", outputCol=\"tags\")\n",
    "hashtag_cv = CountVectorizer(inputCol=hashtag_tokenizer.getOutputCol(), outputCol=\"hashtag_features\", minDF=159)\n",
    "\n",
    "mention_tokenizer = Tokenizer(inputCol=\"mention\", outputCol=\"mentions\")\n",
    "mention_cv = CountVectorizer(inputCol=mention_tokenizer.getOutputCol(), outputCol=\"mention_features\", minDF=159)\n",
    "\n",
    "user_tokenizer = Tokenizer(inputCol=\"from_user\", outputCol=\"users\")\n",
    "user_cv = CountVectorizer(inputCol=user_tokenizer.getOutputCol(), outputCol=\"user_features\", minDF=159)\n",
    "\n",
    "loc_tokenizer = Tokenizer(inputCol=\"location\", outputCol=\"locs\")\n",
    "loc_cv = CountVectorizer(inputCol=loc_tokenizer.getOutputCol(), outputCol=\"loc_features\", minDF=50)\n",
    "\n",
    "pipeline = Pipeline(stages=[term_tokenizer,term_remover,term_cv,hashtag_tokenizer,hashtag_cv,mention_tokenizer, \\\n",
    "                            mention_cv,user_tokenizer, user_cv, loc_tokenizer, loc_cv])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<p><span style=\"font-size: 13.3333px;\"><strong>In this section, we vectorize each feature according to the count threshold above. </strong></span></p>\n",
    "<p><span style=\"font-size: 13.3333px;\"><strong>Fit and tranfer the original dataframe into a set of feature vectors</strong></span></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loading = time.time()\n",
    "\n",
    "model = pipeline.fit(df)\n",
    "Train_X = model.transform(df)\n",
    "\n",
    "getTime(loading)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><span style=\"font-size: 13.3333px;\"><strong>We can now compare our stats with the original paper</strong></span></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><span style=\"font-size: 18px;\"><strong>Original </strong></span></p>\n",
    "![caption](https://github.com/demoonism/TwitterSensor/blob/master/Screenshot/featurecount.JPG?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><span style=\"font-size: 18px;\"><strong> New </strong></span></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### To do..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><span style=\"font-size: 13.3333px;\"><strong>Now we need to concatenate the feature vectors to obtain a feature matrix. Note that we still keep the tweet id in the output because we want to keep a mapping to the original tweet for manual examination</strong></span></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols = [\"term_features\",\"hashtag_features\",\"mention_features\",\"user_features\",\"loc_features\"], outputCol=\"features\")\n",
    "transformed = assembler.transform(Train_X).select(\"tweet_id\",\"features\",\"creat_time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><span style=\"font-size: 13.3333px;\"><strong>We also get the text for each feature by printing out the vocabular. This is useful for manual inspection.</strong></span></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#http://stackoverflow.com/questions/32285699/how-to-get-word-details-from-tf-vector-rdd-in-spark-ml-lib\n",
    "terms_meta = model.stages[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "terms_meta.vocabulary[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Hashtag_meta = model.stages[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Hashtag_meta.vocabulary[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dirty but efficient way to get a list of all feature names. Usful later when we select and examine features.\n",
    "featurelist = model.stages[2].vocabulary+model.stages[4].vocabulary+model.stages[6].vocabulary+model.stages[8].vocabulary+model.stages[10].vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><span style=\"color: #000080;\"><strong>Temporal Split</strong></span></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><span style=\"font-size: 13.3333px;\"><strong>Now we have out feature matrix, it is time to estabulish the training, validation and test set for training the classifier</strong></span></p>\n",
    "<p><span style=\"font-size: 13.3333px;\"><strong>To ensure our classifier generalize to a wide range of features and not simply remeber the past hashtag, we will perform a teppral split to exclude training hashtags in validation and test.</strong></span></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![caption](https://github.com/demoonism/TwitterSensor/blob/master/Screenshot/Capture.JPG?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "term_tokenizer = Tokenizer(inputCol=\"hashtag\", outputCol=\"each_hashtag\")\n",
    "hashtags_df = term_tokenizer.transform(df)\n",
    "\n",
    "hashtag =  hashtags_df.select(\"tweet_id\",\"create_time\",\"each_hashtag\")\n",
    "hash_exploded = hashtag.withColumn('each_hashtag', explode('each_hashtag'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13573239"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hash_exploded.select(\"each_hashtag\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Define topical hashtag list\n",
    "topic_dict = {\n",
    "    \"soccer\":{\"soccer\", \"football\", \"worldcup\", \"sports\", \"futbol\", \"fifa\", \"mls\", \"worldcup2014\", \"epl\", \"sportsroadhouse\", \"sport\", \"adidas\", \"messi\", \"usmnt\", \"arsenal\", \"manchesterunited\", \"nike\", \"ronaldo\", \"manutd\", \"fifaworldcup\", \"foot\", \"ussoccer\", \"sportsbetting\", \"realmadrid\", \"aleague\", \"chelsea\", \"manchester\", \"cr7\", \"footballnews\", \"championsleague\", \"youthsoccer\", \"eplleague\", \"barcelona\", \"brazil2014\", \"soccerproblems\", \"premierleague\", \"brasil2014\", \"soccerlife\", \"cristianoronaldo\", \"uefa\", \"fifa2014\", \"beckham\", \"fifa14\", \"neymar\", \"fussball\", \"soccergirls\", \"barca\", \"manchestercity\", \"league\", \"fútbol\", \"halamadrid\", \"bayern\", \"women\", \"lfc\", \"goalkeeper\", \"everton\", \"bayernmunich\", \"soccerprobs\", \"league1\", \"juventus\", \"nufc\", \"mcfc\", \"cristiano\", \"eurosoccercup\", \"platini\", \"socce\", \"mancity\", \"torontofc\", \"dortmund\", \"derbyday\", \"fifa15\", \"liverpool\", \"league2\", \"ilovesoccer\", \"fcbarcelona\", \"maradona\", \"intermilan\", \"futebol\", \"soccergirlprobs\", \"soccersixfanplayer\", \"realfootball\", \"gunners\", \"confederationscup\", \"worldcupproblems\", \"ballondor\", \"collegesoccer\", \"rooney\", \"flagfootball\", \"realsaltlake\", \"lionelmessi\", \"usavsportugal\", \"europaleague\", \"soccernews\", \"uefachampionsleague\", \"psg\", \"gobrazil\", \"uslpro\", \"wc2014\", \"suarez\", \"bvb\", \"soccerprobz\", \"worldcupqualifiers\", \"torres\", \"footbal\", \"balotelli\", \"nashville\", \"inter\", \"milano\", \"cardiff\", \"jleague\", \"nwsl\", \"ozil\", \"worldcup2014brazil\", \"nycfc\", \"mess\", \"soccernation\", \"pelé\", \"tottenham\", \"ligue1\", \"landondonovan\", \"atletico\", \"worldcup14\", \"torino\", \"soccerislife\", \"fernandotorres\", \"ronaldinho\", \"goldenball\", \"wembley\", \"brazilvscroatia\", \"collegefootball\", \"elclassico\", \"footba\", \"fifa13\", \"soccersunday\", \"englandsoccercup\", \"usasoccer\", \"womensfootball\", \"fcbayern\", \"fifaworldcup2014\", \"usavsgermany\", \"neymarjr\", \"soccersucks\", \"arturovidal\", \"zidane\", \"ballislife\", \"usavsger\", \"mlscup\", \"worldcupfinal\", \"ajax\", \"soccerball\", \"lovesoccer\", \"euro2013\", \"soccergame\", \"premiereleague\", \"mu\", \"lionel\", \"soccermanager\", \"mundial2014\", \"portugalvsgermany\", \"soccerseason\", \"mondiali2014\", \"davidbeckham\", \"redbulls\", \"argvsned\", \"selecao\", \"usavsmex\", \"soccergirlproblems\", \"soccerlove\", \"2014worldcup\", \"soccergrlprobs\", \"germanyvsargentina\", \"zlatan\", \"napoli\", \"muller\", \"confederations_cup\", \"championsleaguefinal\", \"worldcuppredictions\", \"clasico\", \"liverpoolvsrealmadrid\", \"mundialsub17\", \"worldcupbrazil\", \"leaguechamps\", \"arsenalfans\", \"germanyvsalgeria\", \"netherlandsvsargentina\", \"belvsusa\", \"bravsned\", \"mexicovsusa\", \"englandvsuruguay\", \"germanyvsbrazil\", \"brazilvsnetherlands\", \"gervsarg\", \"engvsita\", \"brazilvsgermany\", \"englandvsitaly\", \"espvsned\", \"crcvsned\", \"ghanavsusa\", \"francevsswitzerland\", \"argentinavsgermany\", \"spainvsnetherlands\", \"usavscan\", \"worldcupbrazil2014\", \"brazil2014worldcup\", \"fifaworldcupbrazil\", \"worldcup2018\", \"championleague\"},\n",
    "    \"Natr_Disaster\":{\"sandy\", \"drought\", \"storm\", \"hurricane\", \"tornado\", \"hurricanesandy\", \"earthquake\", \"arthur\", \"julio\", \"manuel\", \"flood\", \"hurricanes\", \"quakelive\", \"hurricaneseason\", \"hurricaneseason\", \"hurricanepride\", \"quake\", \"hurricanekatrina\", \"katrina\", \"floodwarning\", \"eqnz\", \"bertha\", \"tsunami\", \"tsunamimarch\", \"hurricanekid\", \"drought3\", \"hurricanenia\", \"hurricanenation\", \"cholera\", \"hurricanefly\", \"drought13\", \"laquake\", \"typhoon\", \"tsunami2004\", \"ukstorm\", \"hurricaneforever\", \"quakecon2013\", \"prayforchina\", \"quakecon\", \"manuelpellegrini\", \"flood2013\", \"prayforthephilippines\", \"hurricanepreparedness\", \"hurricaneharbor\", \"typhoons\", \"hurricane13\", \"abfloods\", \"ukfloods\", \"hurricaneweek\", \"typhoonmaring\", \"odile\", \"hurricaneprep\", \"phailin\", \"earthquakeph\", \"visayasquake\", \"haiyan\", \"typhoonyolanda\", \"typhoonhaiyan\", \"typhoonaid\", \"typhoonjet\", \"corkfloods\", \"laearthquake\", \"quakecon2014\", \"flood2014\", \"prayforchile\", \"chileearthquake\", \"serbiafloods\", \"tsunamihitsfaisalabad\", \"hurricanearthur\", \"tsunami4nayapakistan\", \"typhoonglenda\", \"hurricanebertha\", \"hurricaneiselle\", \"napaquake\", \"napaearthquake\", \"hurricanemarie\", \"kashmirfloods\", \"hurricaneodile\", \"hurricanegonzalo\", \"hurricaneana\", \"haiyan1year\", \"typhoonhagupit\", \"typhoonruby\"},\n",
    "    \"health\": {\"health\",\"uniteblue\",\"ebola\",\"healthcare\",\"depression\",\"hiv\",\"cdc\",\"crisis\",\"obesity\",\"aids\",\"nurse\",\"flu\",\"alert\",\"publichealth\",\"bandaid30\",\"malaria\",\"disease\",\"fever\",\"antivirus\",\"virus\",\"lagos\",\"unsg\",\"sierraleone\",\"ebolaresponse\",\"ebolaoutbreak\",\"chanyeolvirusday\",\"aids2014\",\"vaccine\",\"mer\",\"homeopathy\",\"msf\",\"allergy\",\"nih\",\"humanitarianheroes\",\"stopthespread\",\"dengue\",\"flushot\",\"epidemic\",\"ebolainatlanta\",\"tuberculosis\",\"westafrica\",\"quarantine\",\"ebolavirus\",\"viruses\",\"kacihickox\",\"emory\",\"meningitis\",\"ebolaczar\",\"enterovirus\",\"pandemic\",\"stopebola\",\"chikungunya\",\"eplague\",\"childhoodobesity\",\"plague\",\"allergyseason\",\"coronavirus\",\"healthworkers\",\"endebola\",\"ebolaqanda\",\"obola\",\"h1n1\",\"aidsfree\",\"factsnotfear\",\"ebolafacts\",\"chickenpox\",\"birdflu\",\"ebolainnyc\",\"dallasebola\",\"ebolachat\",\"eboladallas\",\"childobesity\",\"healthsystems\",\"aidsday\",\"truedepressioniswhen\",\"askebola\",\"depressionawareness\",\"ambervinson\",\"depressionhurts\",\"ninapham\",\"nursesfightebola\",\"mickeyvirus\",\"rotavirus\",\"blackdeath\",\"theplague\",\"fluvaccine\",\"thomasericduncan\",\"plagueinc\",\"stomachvirus\",\"seasonaldepression\",\"mercervirus\",\"beatdepression\",\"aidswalk\",\"depressionproblems\",\"aidswalkny\",\"westnilevirus\",\"depressionkills\",\"smallpox\",\"blackplague\",\"depressionawarenessweek\",\"epoxy\",\"teendepression\",\"fluushots\",\"delpoxespn\",\"virushsality\",\"deepdepression\",\"theamebavirus\",\"fightdepression\",\"plagues\",\"flubug\",\"aidswalkla\",\"bubonicplague\",\"winterdepression\",\"poliovirus\",\"skarfluvvirus\",\"aidsday2014\",\"allergyattack\",\"allergyawarenessweek\",\"pox\",\"spox\",\"theligerplague\",\"thevirus\"},\n",
    "    \"Social_issue\": {\"blacklivesmatter\",\"ferguson\",\"icantbreathe\",\"ericgarner\",\"alllivesmatter\",\"mikebrown\",\"shutitdown\",\"antoniomartin\",\"fergusondecision\",\"nypdlivesmatter\",\"millionsmarchnyc\",\"justice4all\",\"justiceformikebrown\",\"handsupdontshoot\",\"moa\",\"policelivesmatter\",\"berkeleyprotests\",\"thisstopstoday\",\"tamirrice\",\"nojusticenopeace\",\"racism\",\"aurarosser\",\"michaelbrown\",\"thesystemisbroken\",\"blackxmas\",\"policebrutality\",\"deblasio\",\"fergusonoctober\",\"wecantbreathe\",\"justiceforericgarner\",\"every28hours\",\"racist\",\"stoptheparade\",\"enoughisenough\",\"justice\",\"johncrawford\",\"bodycameras\",\"dcferguson\",\"millionsmarch\",\"whereisjustice\",\"blacktwitter\",\"london\",\"police\",\"yamecanse\",\"boston\",\"india\",\"bluelivesmatter\",\"protest\",\"whitelivesmatter\",\"newyork\",\"tcot\",\"justiceforall\",\"equality4all\",\"handsup\",\"whitesilence\",\"economicjustice\",\"solidarity\",\"handsupwalkout\",\"crimingwhilewhite\",\"dontshoot\",\"whiteprivilege\",\"ows\",\"teaparty\",\"wallst\",\"occupy\",\"occupy\",\"p2\",\"tcot\",\"anonymous\",\"teaparty\",\"occupywallstreet\",\"uniteblue\",\"tlot\",\"ferguson\",\"occupylove\"},\n",
    "    \"Celebrity_death\":{\"jamesavery\",\"freshprince\",\"unclephil\",\"freshprinceofbelair\",\"rip\",\"ripjamesavery\",\"thefreshprinceofbelair\",\"robinwilliams\",\"nelsonmandela\",\"philipseymourhoffman\",\"paulwalker\",\"mandela\",\"prayforap\",\"madiba\",\"mayaangelou\",\"rippaulwalker\",\"riprobinwilliams\",\"ripnelsonmandela\",\"ripcorymonteith\",\"ripmandela\",\"ripjoanrivers\",\"riptalia\",\"riplilsnupe\",\"ripleerigby\"}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><span style=\"color: #000080;\"><strong>(Side notes) Saving intermediate data</strong></span></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><span style=\"font-size: 13.3333px;\"><strong>If we want to save intermediate data for any topic, we could do so with the follwing steps (taking the example of natural disaster):</strong></span></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "disaster_ids = hash_exploded.select(hash_exploded.tweet_id).where(hash_exploded.each_hashtag.isin(topic_dict[\"Natr_Disaster\"])).distinct().cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_disaster = df.join(disaster_ids,\\\n",
    "                                 df.tweet_id == disaster_ids.tweet_id,\\\n",
    "                                 \"inner\").select(df.create_time,\\\n",
    "                                                 df.from_id,\\\n",
    "                                                 df.from_user,\\\n",
    "                                                 df.hashtag,\\\n",
    "                                                 df.location,\\\n",
    "                                                 df.mention,\\\n",
    "                                                 df.tweet_id,\\\n",
    "                                                 df.term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "workdir = \"/mnt/4e8ba653-f2f0-4e18-a51e-458026833dee/final_parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Parquet\n",
    "df_disaster.write.save(workdir+\"/Natrual_Disaster_new\", format=\"parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# json\n",
    "df_disaster.write.json(workdir+\"/Natrual_Disaster_json_final\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><span style=\"color: #000080;\"><strong>Hashtag Birthday</strong></span></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><span style=\"font-size: 13.3333px;\"><strong>Hashtag birthday indicates the first timestamp that a particular hashtag appears in the tweet corpus between year 2013 and 2014. We determine this by find the minimum \"create time\" for each hashtag </strong></span></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_birthday = hash_exploded.join(disaster_ids,\\\n",
    "                                 hash_exploded.tweet_id == disaster_ids.tweet_id,\\\n",
    "                                 \"inner\").select(hash_exploded.create_time,\\\n",
    "                                                 hash_exploded.each_hashtag,\\\n",
    "                                                 hash_exploded.tweet_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Find out the \"birthday\", or the earliest appearing time of each hashtag. \n",
    "## (add an extra column of 1 to mark as topical, will be used in a join later)\n",
    "\n",
    "Ordered_Hashtag_set = df_birthday.\\\n",
    "                      groupby(\"each_hashtag\").\\\n",
    "                      agg({\"creat_time\": \"min\"}).\\\n",
    "                      orderBy('min(creat_time)', ascending=True).\\\n",
    "                      withColumn(\"topical\", lit(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Find the total lenth of topical tweets.\n",
    "loading = time.time()\n",
    "time_span = Ordered_Hashtag_set.count()\n",
    "getTime(loading)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get id of the corresponding time split (50% and 60%).\n",
    "train_val_split_Ht = np.floor(np.multiply(time_span, 0.5)).astype(int)\n",
    "val_test_split_Ht =  np.floor(np.multiply(time_span, 0.6)).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Converting to Pandas for random row access.\n",
    "\n",
    "pd_Ordered_Hashtag_set = Ordered_Hashtag_set.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# locate the timestamp of te 50% and 60% cutoff point. Will be used later to divide D.\n",
    "\n",
    "train_val_time = pd_Ordered_Hashtag_set.iloc[train_val_split_Ht]['min(creat_time)']\n",
    "val_test_time = pd_Ordered_Hashtag_set.iloc[val_test_split_Ht]['min(creat_time)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split Hashtags into H_train, H_valid, H_test\n",
    "\n",
    "Train_ids = Ordered_Hashtag_set.select(\"tweet_id\").\\\n",
    "                                     where(col(\"min(creat_time)\") <= train_val_time).distinct()\n",
    "    \n",
    "Valid_ids = Ordered_Hashtag_set.select(\"tweet_id\").\\\n",
    "                                     where((col(\"min(creat_time)\") > train_val_time) & (col(\"min(creat_time)\") <= val_test_time)).distinct()\n",
    "    \n",
    "Test_ids = Ordered_Hashtag_set.select(\"tweet_id\").\\\n",
    "                                     where(col(\"min(creat_time)\") > val_test_time ).distinct()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><span style=\"font-size: 13.3333px;\"><strong>Now we have identified the ids to be used in training, validation and test set, we can proceed to join the id with our feature set to obtain the corresponding data set.</strong></span></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![caption](https://github.com/demoonism/TwitterSensor/blob/master/Screenshot/remove_twit.JPG?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><span style=\"color: #000080;\"><strong>Train-Valid-Test split</strong></span></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Training_set = transformed.select(\"tweet_id\",\"features\").where(col(\"creat_time\") <= train_val_time)\n",
    "\n",
    "Training_set_labled = Training_set.join(Train_ids, Training_set.tweet_id == Train_ids.tweet_id, \"left\").\\\n",
    "                           select(Training_set.tweet_id, Training_set.features, F.when(Train_ids.topical == 1, 1.0).otherwise(0.0).alias(\"label\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loading = time.time()\n",
    "\n",
    "tr_pos_sample = Training_set_labled.where(col(\"label\") == 1.0).count()\n",
    "\n",
    "getTime(loading)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tr_pos_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Raw_Validation_set = transformed.select(\"tweet_id\",\"features\").where((col(\"creat_time\") > train_val_time) & (col(\"creat_time\") <= val_test_time))\n",
    "\n",
    "tr_hashtags_in_vals  = Raw_Validation_set.\\\n",
    "                       join(Train_ids, Raw_Validation_set.tweet_id == Train_ids.tweet_id, \"inner\").\\\n",
    "                       select(Raw_Validation_set.tweet_id)                        \n",
    "\n",
    "Validation_set_staging =  Raw_Validation_set.\\\n",
    "                          join(tr_hashtags_in_vals, Raw_Validation_set.tweet_id == tr_hashtags_in_vals.tweet_id, \"left_outer\").\\\n",
    "                          toDF(\"tweet_id\",\"features\",\"new_id\")\n",
    "#### This is a huge assssss fucking bug. direct select would remove null type.\n",
    "\n",
    "Validation_set =  Validation_set_staging.select(col(\"tweet_id\"),col(\"features\")).where(col(\"new_id\").isNull())\n",
    "\n",
    "\n",
    "Validation_set_labled = Validation_set.join(Valid_ids, Validation_set.tweet_id == Valid_ids.tweet_id, \"left\").\\\n",
    "                           select(Validation_set.tweet_id, Validation_set.features, F.when(Valid_ids.topical == 1, 1.0).otherwise(0.0).alias(\"label\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val_pos_sample = Validation_set_labled.where(col(\"label\") == 1.0).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val_pos_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Raw_Test_set = transformed.select(\"tweet_id\",\"features\").where(col(\"creat_time\") > val_test_time)\n",
    "\n",
    "tr_hashtags_in_test  = Raw_Test_set.\\\n",
    "                       join(Train_ids, Raw_Test_set.tweet_id == Train_ids.tweet_id, \"inner\").\\\n",
    "                       select(Raw_Test_set.tweet_id)\n",
    "\n",
    "Test_set_staging =  Raw_Test_set.\\\n",
    "                          join(tr_hashtags_in_test, Raw_Test_set.tweet_id == tr_hashtags_in_test.tweet_id, \"left_outer\").\\\n",
    "                          toDF(\"tweet_id\",\"features\",\"new_id\")\n",
    "\n",
    "Test_set =  Test_set_staging.select(col(\"tweet_id\"),col(\"features\")).where(col(\"new_id\").isNull())\n",
    "\n",
    "\n",
    "Test_set_labled = Test_set.join(Test_ids, Test_set.tweet_id == Test_ids.tweet_id, \"left\").\\\n",
    "                           select(Test_set.tweet_id, Test_set.features, F.when(Test_ids.topical == 1, 1.0).otherwise(0.0).alias(\"label\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "te_pos_sample = Test_set_labled.where(col(\"label\") == 1.0).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "te_pos_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling data to balance label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Concatenate pos and neg training samples to form the final training set.\n",
    "\n",
    "Input = Training_set_labled.sampleBy(\"label\", fractions={0.0: 0.05, 1.0: 1}, seed=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Input.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><span style=\"color: #000080;\"><strong><span style=\"font-family: georgia, palatino, serif; font-size: 24pt;\">Step Three: Training Classifier</span></strong></span></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import ChiSqSelector\n",
    "from pyspark.ml.linalg import Vectors\n",
    "## note that the 100 here is just a dummy varible. The actual number of features to use will be part of the grid search.\n",
    "\n",
    "selector = ChiSqSelector(numTopFeatures=100, featuresCol=\"features\",\n",
    "                         outputCol=\"selectedFeatures\", labelCol=\"label\").cache()\n",
    "model = selector.fit(Input)\n",
    "result = model.transform(Input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result.where(col(\"label\") == 1.0).show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><span style=\"color: #000080;\"><strong>Train logistic regression and Hyper Parameter Tunning</strong></span></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><span style=\"font-size: 13.3333px;\"><strong>We are tunning two hyperparameters for the logistic regression, namly number of features and L2 penalty</strong></span></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "blor = LogisticRegression(maxIter=5 featuresCol='selectedFeatures', labelCol='label')\n",
    "patk = BinaryClassificationEvaluator()\n",
    "#pipeline = Pipeline(stages=[selector, blor])\n",
    "pipeline = Pipeline(stages=[blor])\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(selector.numTopFeatures, [10, 100, 1000, 10000, 50000, 1000000, 5000000]) \\\n",
    "    .addGrid(blor.regParam, [0.000001, 0.00001, 0.0001, 0.001, 0.01, 0.1, 0.5, 0.8, 1, 1.5, 2.0, 2.5, 10, 100]) \\   \n",
    "    .build()\n",
    "\n",
    "lr.regParam, [0.0001, 0.001, 0.01, 0.1, 0.15, 0.2, 0.3]\n",
    "    \n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=BinaryClassificationEvaluator(),\n",
    "                          numFolds=3)  # use 3+ folds in practice\n",
    "\n",
    "# Run cross-validation, and choose the best set of parameters.\n",
    "cvModel = crossval.fit(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "predictionAndLabels = results.select('probability', 'prediction', 'prediction').map(lambda x: (x[1], x[2]))\n",
    "metrics = MulticlassMetrics(predictionAndLabels)\n",
    "metrics.confusionMatrix().toArray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = result.select(\"probability\").toDF(\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorSlicer\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql.types import Row\n",
    "\n",
    "staging = slicer1.transform(result)\n",
    "output =  slicer2.transform(staging)\n",
    "\n",
    "output.select(\"a\", \"0_prob\", \"1_prob\").orderBy('1_prob', ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#As of Spark 2.0 ml and mllib API are no longer compatible and the latter one is going towards deprecation and removal. If you still need this you'll have to convert ml.Vectors to mllib.Vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib import linalg as mllib_linalg\n",
    "from pyspark.ml import linalg as ml_linalg\n",
    "\n",
    "def as_old(v):\n",
    "    if isinstance(v, ml_linalg.SparseVector):\n",
    "        return mllib_linalg.SparseVector(v.size, v.indices, v.values)\n",
    "    if isinstance(v, ml_linalg.DenseVector):\n",
    "        return mllib_linalg.DenseVector(v.values)\n",
    "    raise ValueError(\"Unsupported type {0}\".format(type(v)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "#from pyspark.mllib.evaluation import RegressionMetrics, RankingMetrics\n",
    "\n",
    "# Compute raw scores on the test set\n",
    "\n",
    "predictionAndLabels = Valid_RDD.map(lambda lp: (float(model.predict(lp.features)), float(lp.label)))\n",
    "\n",
    "Pred = predictionAndLabels.map(lambda x:x[0])\n",
    "Truth = predictionAndLabels.map(lambda x:x[1])\n",
    "Pred_truth = (b.take(100), c.take(100))\n",
    "predictionAndLabels = sc.parallelize([Pred_truth])\n",
    "\n",
    "\n",
    "# Instantiate metrics object\n",
    "## Ranking metrics ONLY takes tuple of list (pred, groundtruth)\n",
    "metrics = RankingMetrics(predictionAndLabels)\n",
    "print(\"Precision @ k = %s\" % metrics.precisionAt(100)) \n",
    "\n",
    "#print(\"Mean Average precision = %s\" % metrics.meanAveragePrecision)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.predictAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictionAndLabels.take(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
