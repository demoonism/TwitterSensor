{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Notebook property setup.\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import udf, col\n",
    "\n",
    "import sys\n",
    "import time\n",
    "import os.path\n",
    "import json\n",
    "from datetime import datetime\n",
    "from operator import add\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import preprocessor as p\n",
    "import string\n",
    "\n",
    "## Enable inline graphs\n",
    "%matplotlib inline\n",
    "\n",
    "## Display precision for pandas dataframe\n",
    "pd.set_option('precision',10)\n",
    "\n",
    "## Set up language classifier, used to filtered out non-English files\n",
    "import langid\n",
    "langid.set_languages(['de','fr','it','en','zh','ar','ja','ko', 'es','ms','tr','hi','bn','pa'])\n",
    "\n",
    "workdir = \"/mnt/381c2633-4d72-4555-9be8-19e922cce4a1/parquet_out\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading DATA\n",
    "\n",
    "###### Raw data are saved as json.gz format. We need to load and parse these data into spark RDD. Note that, the sc.textFile function's input directory could be either a file or a directory. Spark context will create partitions automatically. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 2013 data\n",
    "\n",
    "data_2013_raw = sc.textFile(\"/mnt/1e69d2b1-91a9-473c-a164-db90daf43a3d/Backup_tw_2013_7/2013-07/2013-07-*,\\\n",
    "/mnt/1e69d2b1-91a9-473c-a164-db90daf43a3d/Backup_tw_2013_8/2013-08/2013-08-*,\\\n",
    "/mnt/1e69d2b1-91a9-473c-a164-db90daf43a3d/Backup_tw_2013_9/2013-09/2013-09-*,\\\n",
    "/home/danielshi/Backup_tw_2013_2/2013-02/2013-02-*,\\\n",
    "/home/danielshi/Backup_tw_2013_3/2013-03/2013-03-*,\\\n",
    "/home/danielshi/Backup_tw_2013_6/2013-06/2013-06-*,\\\n",
    "/mnt/2b53fde0-61da-4eeb-a038-9910540ff9ad/Backup_tw_2013_10/2013-10/2013-10-*,\\\n",
    "/mnt/2b53fde0-61da-4eeb-a038-9910540ff9ad/Backup_tw_2013_11/2013-11/2013-11-*,\\\n",
    "/mnt/2b53fde0-61da-4eeb-a038-9910540ff9ad/Backup_tw_2013_12/2013-12/2013-12-*,\\\n",
    "/mnt/73dc2fdb-c49c-484c-bef8-7a6fc6abbc70/Backup_tw_2013_4/2013-04/2013-04-*,\\\n",
    "/mnt/73dc2fdb-c49c-484c-bef8-7a6fc6abbc70/Backup_tw_2013_5/2013-05/2013-05-*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 2014 data\n",
    "\n",
    "data_2014_raw = sc.textFile(\"/mnt/73dc2fdb-c49c-484c-bef8-7a6fc6abbc70/Backup_tw_2014_1/2014-01/2014-01-*,\\\n",
    "/mnt/381c2633-4d72-4555-9be8-19e922cce4a1/Backup_tw_2014_2/2014-02/2014-02-*,\\\n",
    "/mnt/381c2633-4d72-4555-9be8-19e922cce4a1/Backup_tw_2014_3/2014-03/2014-03-*,\\\n",
    "/mnt/381c2633-4d72-4555-9be8-19e922cce4a1/Backup_tw_2014_4/2014-04/2014-04-*,\\\n",
    "/mnt/b93e71ec-8ddf-4033-bd42-770c05bc68aa/Backup_tw_2014_5/2014-05/2014-05-*,\\\n",
    "/mnt/b93e71ec-8ddf-4033-bd42-770c05bc68aa/Backup_tw_2014_6/2014-06/2014-06-*,\\\n",
    "/mnt/b93e71ec-8ddf-4033-bd42-770c05bc68aa/Backup_tw_2014_7/2014-07/2014-07-*,\\\n",
    "/mnt/4e8ba653-f2f0-4e18-a51e-458026833dee/Backup_tw_2014_8/2014-08/2014-08-*,\\\n",
    "/mnt/4e8ba653-f2f0-4e18-a51e-458026833dee/Backup_tw_2014_9/2014-09/2014-09-*,\\\n",
    "/mnt/4e8ba653-f2f0-4e18-a51e-458026833dee/Backup_tw_2014_10/2014-10/2014-10-*,\\\n",
    "/mnt/66e695cd-1a0c-4e3b-9a50-55e01b788529/Backup_tw_2014_11/2014-11/2014-11-*,\\\n",
    "/mnt/66e695cd-1a0c-4e3b-9a50-55e01b788529/Backup_tw_2014_12/2014-12/2014-12-*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Helper function to keep track the run time of a spark ops.\n",
    "def getTime(start):\n",
    "    sec = time.time() - start\n",
    "    m, s = divmod(sec, 60)\n",
    "    h, m = divmod(m, 60)\n",
    "    print('Spark operation takes - %d:%02d:%02d which is %d seconds in total' % (h,m,s,sec))\n",
    "    \n",
    "    \n",
    "# Remove invalid tweet which has length less than 1000.\n",
    "def ValidJson(d):\n",
    "    return len(d) > 1000\n",
    "\n",
    "# load json object, if a line is invalid, substitute as an empty dict (which has len() == 0 )\n",
    "def loadJson(d):\n",
    "    try:\n",
    "        js = json.loads(d)\n",
    "        \n",
    "    except ValueError as e:\n",
    "        js = {}\n",
    "        \n",
    "    except Exception:\n",
    "        js = {}\n",
    "        \n",
    "    return js\n",
    "\n",
    "# Some tweet does not contain the 'lang' key, removing as invalid.\n",
    "def containsLang(d):\n",
    "    return 'lang' in d\n",
    "\n",
    "# Raw filter using twitter's default language detection. Note that the accuracy is very low, therefore we need to apply a \n",
    "# second level language detection to further remove non-Eng tweets. \n",
    "def Eng_Label(d):\n",
    "    return d['lang'] == 'en'\n",
    "\n",
    "# Convert timestamp to unix time string, usful when finding hashtag bithdates later.\n",
    "def getUnixTimeStamp(stamp):\n",
    "    d = datetime.strptime(stamp,'%a %b %d %H:%M:%S +0000 %Y')\n",
    "    unixtime = time.mktime(d.timetuple())\n",
    "    return unixtime\n",
    "\n",
    "\n",
    "# Parse out the releavant attributes from raw tweet to save memory,also converting hastags and mentions to space-separate lists.\n",
    "def RawParser(d):\n",
    "    processed = {\"from_user\":d['user']['screen_name'],\n",
    "                 \"from_id\":d['user']['id'],\n",
    "                 ## Split hashtag, we only want the text in hashtag, discard indices.\n",
    "                 \"tweet_id\":d['id'],\n",
    "                 \"hashtag\":\" \".join([hash_string['text'] for hash_string in d['entities']['hashtags']]), \n",
    "                 ## Split terms in tweet text, remove \\n and \\r\n",
    "                 \"term\": d['text'],\n",
    "                 ## append loc_ to each word in location\n",
    "                 #\"location\":['loc_' + s for s in d['user']['location'].split(\" \")],\n",
    "                 \"location\":d['user']['location'],\n",
    "                 ## mention ids\n",
    "                 \"mention\":\" \".join([mention['screen_name'] for mention in d['entities']['user_mentions']]),\n",
    "                 \"create_time\":getUnixTimeStamp(d['created_at'])\n",
    "                }\n",
    "    return processed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FeatureRDD_2013 = data_2013_raw.filter(ValidJson).map(loadJson).filter(lambda x: len(x) > 1).filter(containsLang).filter(Eng_Label).map(RawParser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FeatureRDD_2014 = data_2014_raw.filter(ValidJson).map(loadJson).filter(lambda x: len(x) > 1).filter(containsLang).filter(Eng_Label).map(RawParser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Year 2013 contains \"+ str(FeatureRDD_2013.getNumPartitions())+\" file partitions\")\n",
    "print(\"Year 2014 contains \"+ str(FeatureRDD_2014.getNumPartitions())+\" file partitions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Define Dataframe schema. Converting RDD to dataframe.\n",
    "schema = StructType([StructField('create_time', DoubleType(), False),\n",
    "                     StructField('from_id', StringType(), False),\n",
    "                     StructField('from_user', StringType(), False),\n",
    "                     StructField('hashtag', StringType(), True),\n",
    "                     StructField('location', StringType(), True),\n",
    "                     StructField('mention', StringType(), True),\n",
    "                     StructField('term', StringType(), True),\n",
    "                     StructField('tweet_id', StringType(), False)\n",
    "                    ])\n",
    "Feature_df_2013 = sqlContext.createDataFrame(FeatureRDD_2013, schema)\n",
    "Feature_df_2014 = sqlContext.createDataFrame(FeatureRDD_2014, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Feature_df_2013.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Feature_df_2014.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Saving parsed 2013 data to parquet, save space, better performance\n",
    "#Feature_df_2013.write.save(workdir+\"/2013_Raw_Eng.parquet\", format=\"parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Saving parsed 2014 data to parquet, save space, better performance\n",
    "#Feature_df_2014.write.save(workdir+\"/2014_Raw_Eng.parquet\", format=\"parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading parquet into rdd again for non-english filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Ref: https://www.mail-archive.com/user@spark.apache.org/msg28820.html    changing user permission.\n",
    "All_RDD2013 = spark.read.parquet(workdir+\"/2013_Raw_parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "All_RDD2014 = spark.read.parquet(workdir+\"/2014_Raw_parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Saving dataframes above as an intermediate json file. Unless you need additional attributes, this should be the data the you work with for later processing steps. the raw data is not longer releavant at this time.\n",
    "##### It is much more difficult to perfrom custom map reduce on dataframe; it is easier to work with RDDs. Also, It is easier to save data as json than load into rdd comparing to converting dataframe to RDD directly (will get Row type, not primative RDD).  Therefore, we save the same data as json format as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#All_RDD2013.write.json(workdir_eng+\"/2013_Raw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#All_RDD2014.write.json(workdir_eng+\"/2014_Raw\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilizing the langid package to filter out tweets which contains non english char in tweet terms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### There are various lanuage detection libraries for python, the problem comes down to speed and accuracy. Out of all packages I tested (apache Tika, langid, lang detect, guess-language, textblob), two stand out the most: langid and textblob. \n",
    "\n",
    "##### Langid utilize multithreading, and works great on short text (ex. tweet terms),however, the accuracy decreases when multiple languages are mixed up in the text. It takes 0.0003 seconds to check one line. It has a major drawback: the multi-threading module in this package does not seem to work well with spark. In other words, if we try to concurrently run multiple python jobs with this lib, it will create deadlocks. It will be interesting to look into the source code of this lib to understand why.\n",
    "\n",
    "##### Textblob is based on NLTK, and it delivers the best accuracy among other packages available. When multiple lanuage appears in the text, the majority wins (bayesian). However, it takes an average of 0.1 seconds to process one line, which is way too slow for big data practice.\n",
    "\n",
    "##### considering accuracy and efficiency, I used Langid here. Since it does not run well with spark, I create a separate bash script to trigger 2 python instance to process data for 2013 and 2014. \n",
    "\n",
    "##### SHOULD CONSIDERING RUN SPARK WITH TEXTBLOB FOR BETTER ACCURACY IF TIME PERMITTED. (~ Takes 8 days)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run this command to execute 2013_Eng_Filter.py and 2014_Eng_Filter.py. It takes 5 full days to finish the parsing. (if line magic does not work, run it in terminal instead.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## !bash Eng_Filter.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### This script will filter the json data and return English tweets only. note that, the location and hashtag may still contain non-english words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 2013 data\n",
    "data_2013_Eng = sc.textFile(\"/mnt/1e69d2b1-91a9-473c-a164-db90daf43a3d/Eng_Json/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 2014 data\n",
    "data_2014_Eng = sc.textFile(\"/mnt/2b53fde0-61da-4eeb-a038-9910540ff9ad/Eng_Json/*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 2013 data contains 360048691 valid english tweets, 2014 data contains 455285530 valid engish tweets. We have a total of comapring to 829, 026, 458 in the Paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Now we have English data, let's go ahead and clean it a little bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#1. If the tweet has an empty location field. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "invalid = data_2013_Eng.map(loadJson).filter(lambda x: \"location\" not in x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loading = time.time()\n",
    "\n",
    "invalid.count()\n",
    "\n",
    "getTime(loading)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import preprocessor as p\n",
    "import string\n",
    "\n",
    "def translating(x):\n",
    "    return x.encode('utf-8').lower().translate(None, string.punctuation)\n",
    "\n",
    "def Cleansing(d):\n",
    "    txt = p.clean(d['term'].encode('ascii', 'ignore')).replace(\":\", \"\").lower()\n",
    "\n",
    "    if d['location'] == None:\n",
    "        loc_term = \"empty_location\"\n",
    "    elif d['location'].strip(' ') == '':\n",
    "        loc_term = \"empty_location\"\n",
    "    else:\n",
    "        loc_term = 'loc_' + \"_\".join(map(translating, d['location'].strip(' ').split(\" \")))\n",
    "        \n",
    "    if txt == None:\n",
    "        terms = \"empty_tweet\"\n",
    "    elif txt.strip(' ') == '':\n",
    "        terms = \"empty_tweet\"\n",
    "    else:\n",
    "        terms = txt.encode('utf-8').translate(None, string.punctuation).strip(' ')\n",
    "        \n",
    "    processed = {\"from_user\":d['from_user'],\n",
    "                 \"from_id\":d['from_id'],\n",
    "                 \"tweet_id\":d['tweet_id'],\n",
    "                 \"hashtag\":d['hashtag'], \n",
    "                 \"term\": terms,\n",
    "                 \"location\":loc_term,\n",
    "                 \"mention\":d['mention'],\n",
    "                 \"create_time\":d['create_time']\n",
    "                }\n",
    "    return processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cleanRDD_2013 = data_2013_Eng.map(loadJson).filter(lambda x: \"location\" in x).map(Cleansing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cleanRDD_2014 = data_2014_Eng.map(loadJson).filter(lambda x: \"location\" in x).map(Cleansing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Define Dataframe schema.\n",
    "schema = StructType([StructField('create_time', DoubleType(), False),\n",
    "                     StructField('from_id', StringType(), False),\n",
    "                     StructField('from_user', StringType(), False),\n",
    "                     StructField('hashtag', StringType(), True),\n",
    "                     StructField('location', StringType(), True),\n",
    "                     StructField('mention', StringType(), True),\n",
    "                     StructField('term', StringType(), True),\n",
    "                     StructField('tweet_id', StringType(), False)\n",
    "                    ])\n",
    "Final_Feature_df_2013 = sqlContext.createDataFrame(cleanRDD_2013, schema)\n",
    "Final_Feature_df_2014 = sqlContext.createDataFrame(cleanRDD_2014, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Final_Feature_df_2014.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "workdir = \"/mnt/4e8ba653-f2f0-4e18-a51e-458026833dee/final_parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Saving 2013 data to parquet, save space, better performance\n",
    "Final_Feature_df_2013.write.save(workdir+\"/2013_Eng_parquet_clean\", format=\"parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Saving 2014 data to parquet, save space, better performance\n",
    "Final_Feature_df_2014.write.save(workdir+\"/2014_Eng_parquet_fixed\", format=\"parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
