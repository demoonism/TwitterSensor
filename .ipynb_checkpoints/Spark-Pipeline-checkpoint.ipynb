{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><span style=\"font-size: 36pt; font-family: georgia, palatino, serif; color: #800000;\">Learning Topical Social Sensors</span></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><span style=\"font-family: georgia, palatino, serif; font-size: 24pt; color: #800000;\">Step One: Pre-processing</span></p>\n",
    "\n",
    "###### <p><em>The original Twitter data were collected over 2 years, which contains over 2TB compressed data. It consists of hundreds of millions lines of tweets. Each valid tweet looks like this:</em></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><span style=\"font-size: 10pt;\"><strong>Example</strong></span></p>\n",
    "<p><span style=\"font-size: 8pt;\"><strong>{</strong>\"created_at\":\"Thu Jan 31 12:58:06 +0000 2013\",</span><br /><span style=\"font-size: 8pt;\"> \"id\":296965581582786560,</span><br /><span style=\"font-size: 8pt;\"> \"id_str\":\"296965581582786560\",</span><br /><span style=\"font-size: 8pt;\"> \"text\":\"Im ready for whatever\",</span><br /><span style=\"font-size: 8pt;\"> \"source\":\"\\u003ca href=\\\"http:\\/\\/twitter.com\\/download\\/iphone\\\" rel=\\\"nofollow\\\"\\u003eTwitter for iPhone\\u003c\\/a\\u003e\",</span><br /><span style=\"font-size: 8pt;\"> \"truncated\":false,</span><br /><span style=\"font-size: 8pt;\"> \"in_reply_to_status_id\":null,</span><br /><span style=\"font-size: 8pt;\"> \"in_reply_to_status_id_str\":null,</span><br /><span style=\"font-size: 8pt;\"> \"in_reply_to_user_id\":null,</span><br /><span style=\"font-size: 8pt;\"> \"in_reply_to_user_id_str\":null,</span><br /><span style=\"font-size: 8pt;\"> \"in_reply_to_screen_name\":null,</span><br /><span style=\"font-size: 8pt;\"> \"user\":{</span><br /><span style=\"font-size: 8pt;\"> \"id\":1059349532,</span><br /><span style=\"font-size: 8pt;\"> \"id_str\":\"1059349532\",</span><br /><span style=\"font-size: 8pt;\"> \"name\":\"Don Dada\",</span><br /><span style=\"font-size: 8pt;\"> \"screen_name\":\"ImDatNiggaBD\",</span><br /><span style=\"font-size: 8pt;\"> \"location\":\"South Side Of Little Rock\",</span><br /><span style=\"font-size: 8pt;\"> \"url\":null,</span><br /><span style=\"font-size: 8pt;\"> \"description\":\"Weed Smoker (Kush)\",</span><br /><span style=\"font-size: 8pt;\"> \"protected\":false,</span><br /><span style=\"font-size: 8pt;\"> \"followers_count\":109,</span><br /><span style=\"font-size: 8pt;\"> \"friends_count\":110,</span><br /><span style=\"font-size: 8pt;\"> \"listed_count\":0,</span><br /><span style=\"font-size: 8pt;\"> \"created_at\":\"Fri Jan 04 02:37:28 +0000 2013\",</span><br /><span style=\"font-size: 8pt;\"> \"favourites_count\":14,</span><br /><span style=\"font-size: 8pt;\"> \"utc_offset\":null,</span><br /><span style=\"font-size: 8pt;\"> \"time_zone\":null,</span><br /><span style=\"font-size: 8pt;\"> \"geo_enabled\":false,</span><br /><span style=\"font-size: 8pt;\"> \"verified\":false,</span><br /><span style=\"font-size: 8pt;\"> \"statuses_count\":1312,</span><br /><span style=\"font-size: 8pt;\"> \"lang\":\"en\",</span><br /><span style=\"font-size: 8pt;\"> \"contributors_enabled\":false,</span><br /><span style=\"font-size: 8pt;\"> \"is_translator\":false,</span><br /><span style=\"font-size: 8pt;\"> \"profile_background_color\":\"C0DEED\",</span><br /><span style=\"font-size: 8pt;\"> \"profile_background_image_url\":\"http:\\/\\/a0.twimg.com\\/images\\/themes\\/theme1\\/bg.png\",</span><br /><span style=\"font-size: 8pt;\"> \"profile_background_image_url_https\":\"https:\\/\\/si0.twimg.com\\/images\\/themes\\/theme1\\/bg.png\",</span><br /><span style=\"font-size: 8pt;\"> \"profile_background_tile\":false,</span><br /><span style=\"font-size: 8pt;\"> \"profile_image_url\":\"http:\\/\\/a0.twimg.com\\/profile_images\\/3184813228\\/d6d3a95d902f088f412cf1bd90c126c7_normal.jpeg\",</span><br /><span style=\"font-size: 8pt;\"> \"profile_image_url_https\":\"https:\\/\\/si0.twimg.com\\/profile_images\\/3184813228\\/d6d3a95d902f088f412cf1bd90c126c7_normal.jpeg\",</span><br /><span style=\"font-size: 8pt;\"> \"profile_banner_url\":\"https:\\/\\/si0.twimg.com\\/profile_banners\\/1059349532\\/1359068332\",</span><br /><span style=\"font-size: 8pt;\"> \"profile_link_color\":\"0084B4\",</span><br /><span style=\"font-size: 8pt;\"> \"profile_sidebar_border_color\":\"C0DEED\",</span><br /><span style=\"font-size: 8pt;\"> \"profile_sidebar_fill_color\":\"DDEEF6\",</span><br /><span style=\"font-size: 8pt;\"> \"profile_text_color\":\"333333\",</span><br /><span style=\"font-size: 8pt;\"> \"profile_use_background_image\":true,</span><br /><span style=\"font-size: 8pt;\"> \"default_profile\":true,</span><br /><span style=\"font-size: 8pt;\"> \"default_profile_image\":false,</span><br /><span style=\"font-size: 8pt;\"> \"following\":null,</span><br /><span style=\"font-size: 8pt;\"> \"follow_request_sent\":null,</span><br /><span style=\"font-size: 8pt;\"> \"notifications\":null},</span><br /><span style=\"font-size: 8pt;\"> \"geo\":null,</span><br /><span style=\"font-size: 8pt;\"> \"coordinates\":null,</span><br /><span style=\"font-size: 8pt;\"> \"place\":null,</span><br /><span style=\"font-size: 8pt;\"> \"contributors\":null,</span><br /><span style=\"font-size: 8pt;\"> \"retweet_count\":0,</span><br /><span style=\"font-size: 8pt;\"> \"entities\":{\"hashtags\":[],</span><br /><span style=\"font-size: 8pt;\"> \"urls\":[],</span><br /><span style=\"font-size: 8pt;\"> \"user_mentions\":[]},</span><br /><span style=\"font-size: 8pt;\"> \"favorited\":false,</span><br /><span style=\"font-size: 8pt;\"> \"retweeted\":false,</span><br /><span style=\"font-size: 8pt;\"> \"lang\":\"en\"<strong>}</strong></span></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### <p><em>Obviously, not all data are relevant to our analysis. As according to the paper, the Only releavant fields in our features are</em></p>\n",
    "<p><em><strong>{Hashtags}, {From_User}, {Create_Time}, {Location}, {Mentions}</strong></em></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### To filter out irrelavant data and keep our input clean, we have a two-step process:\n",
    "1. run the Pre-processing.py to parse all data. \n",
    "2. run the filterEng.py to filterout all non-English Tweets.\n",
    "\n",
    "###### The resulting data looks like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><strong>Processed-tweet:</strong></p>\n",
    "<p><strong>{</strong>u'Create_time': 1359737884.0,<br /> u'from_id': 87151732,<br /> u'from_user': u'ishiPTI',<br /> u'hashtag': u'',<br /> u'location': u'loc_dha_lahore_cantt_',<br /> u'mention': u'BushraShekhani',<br /> u'term': u'I am ready for whatever',<br /> u'tweet_id': 297312861586325504<strong>}</strong></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### <p><em>Now we have a small (sort of) and clean dataset to work with, it is time to move on to spark to perform some reall analysis.</em></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><span style=\"font-size: 24pt; color: #800000; font-family: georgia, palatino, serif;\">Step Two: Feature Processing</span></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "###### We need to turn the raw json data into feature matrix. There are two keys here: 1. data processing must be extremly efficient and 2. The resulting matrix must be sparse. These are achieved through the following pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Notebook property setup.\n",
    "\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import udf, col, lit, monotonically_increasing_id, explode\n",
    "\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.param import Param, Params\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer, IDF, StopWordsRemover, CountVectorizer, VectorAssembler\n",
    "\n",
    "import sys\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import os.path\n",
    "import json\n",
    "import time\n",
    "#import simplejson as json\n",
    "from datetime import datetime\n",
    "from operator import add\n",
    "\n",
    "## Helper function to keep track the run time of a spark ops.\n",
    "\n",
    "def getTime(start):\n",
    "    sec = time.time() - start\n",
    "    m, s = divmod(sec, 60)\n",
    "    h, m = divmod(m, 60)\n",
    "    print('Spark operation takes - %d:%02d:%02d which is %d seconds in total' % (h,m,s,sec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "## Enable inline graphs\n",
    "%matplotlib inline\n",
    "\n",
    "## Display precision for pandas dataframe\n",
    "pd.set_option('precision',10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading DATA\n",
    "\n",
    "###### After preprocessing, data are saved as json.gz format. We need to load and parse these data into spark RDD. Note that, the sc.textFile function's input directory could be either a file or a directory. Spark context will create partitions automatically. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# full\n",
    "#data_raw = sc.textFile('/mnt/66e695cd-1a0c-4e3b-9a50-55e01b788529/Tweet_Output/Sample')\n",
    "data_raw = sc.textFile('/mnt/66e695cd-1a0c-4e3b-9a50-55e01b788529/Tweet_Output/small_sample')\n",
    "data = data_raw.map(lambda line: json.loads(line))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Taking a look at the (parsed) first line of our input files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample = data.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turning to dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### An RDD (Resilient Distributed Dataset) is more of a blackbox of data that cannot be optimized as the operations that can be performed against it, are not as constrained. (Available in Spark since 1.0)\n",
    "\n",
    "###### A dataframe is a table, or two-dimensional array-like structure, in which each column contains measurements on one variable, and each row contains one case. Therefore, a DataFrame has additional metadata due to its tabular format, which allows Spark to run certain optimizations on the finalized query. (Added since 1.3)\n",
    "\n",
    "###### In short, you are able to write traditional map-reduce type of code on both RDD and Dataframe, but Dataframe also support SQL command and built-in analytical functions. For performance conideration, we are turning our RDD into Dataframes first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Define Dataframe schema.\n",
    "schema = StructType([StructField('HashTag_Birthday', DoubleType(), False),\n",
    "                     StructField('from_id', IntegerType(), False),\n",
    "                     StructField('from_user', StringType(), False),\n",
    "                     StructField('hashtag', StringType(), True),\n",
    "                     StructField('location', StringType(), True),\n",
    "                     StructField('mention', StringType(), True),\n",
    "                     StructField('term', StringType(), True),\n",
    "                     StructField('tweet_id', StringType(), False)                     \n",
    "                    ])\n",
    "df = sqlContext.createDataFrame(data, schema)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Input are shown as tabular form (Dataframe) below. Note that hashtag field is happend to be null for the first few records. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---------+----------+-------+--------------------+--------------+--------------------+------------------+\n",
      "|HashTag_Birthday|  from_id| from_user|hashtag|            location|       mention|                term|          tweet_id|\n",
      "+----------------+---------+----------+-------+--------------------+--------------+--------------------+------------------+\n",
      "|   1.359737884E9| 87151732|   ishiPTI|       |loc_dha_lahore_ca...|BushraShekhani|bushrashekhani ye...|297312861586325504|\n",
      "|   1.359737885E9|945063858|F5Everyday|       |  loc_brandon_ms_usa|       uziha06|rt uziha06 a man’...|297312865747083264|\n",
      "|   1.359737885E9|231813876|     k30ji|       |            loc_kmkn|     belongfr7|    belongfr7 belong|297312865747103745|\n",
      "|   1.359737885E9|404571997|0infelnity|       |                loc_|  konnichimaru|        konnichimaru|297312865751281664|\n",
      "|   1.359737885E9|377993505|   nftnhni|       |      loc_singapore_|      asmooday|rt asmooday you w...|297312865772240896|\n",
      "+----------------+---------+----------+-------+--------------------+--------------+--------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Now we need to transform the textual features into a sparse vector to be piped into our learning algorithm later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizing user, hashtag, location, mention, term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "term_tokenizer = Tokenizer(inputCol=\"term\", outputCol=\"words\")\n",
    "term_remover = StopWordsRemover(inputCol=term_tokenizer.getOutputCol(), outputCol=\"filtered\")\n",
    "term_cv = CountVectorizer(inputCol=term_remover.getOutputCol(), outputCol=\"term_features\", vocabSize=500, minTF= 2, minDF=1)\n",
    "#term_pipeline = Pipeline(stages=[term_tokenizer, term_remover,term_cv])\n",
    "\n",
    "\n",
    "hashtag_tokenizer = Tokenizer(inputCol=\"hashtag\", outputCol=\"tags\")\n",
    "hashtag_cv = CountVectorizer(inputCol=hashtag_tokenizer.getOutputCol(), outputCol=\"hashtag_features\", vocabSize=500, minTF = 2, minDF=1)\n",
    "#hashtag_pipeline = Pipeline(stages=[hashtag_tokenizer,hashtag_cv])\n",
    "\n",
    "mention_tokenizer = Tokenizer(inputCol=\"mention\", outputCol=\"mentions\")\n",
    "mention_cv = CountVectorizer(inputCol=mention_tokenizer.getOutputCol(), outputCol=\"mention_features\", vocabSize=100, minTF= 2, minDF=1)\n",
    "#mention_pipeline = Pipeline(stages=[mention_tokenizer, mention_cv])\n",
    "\n",
    "user_tokenizer = Tokenizer(inputCol=\"from_user\", outputCol=\"users\")\n",
    "user_cv = CountVectorizer(inputCol=user_tokenizer.getOutputCol(), outputCol=\"user_features\", vocabSize=100, minTF= 2, minDF=1)\n",
    "\n",
    "loc_tokenizer = Tokenizer(inputCol=\"location\", outputCol=\"locs\")\n",
    "loc_cv = CountVectorizer(inputCol=loc_tokenizer.getOutputCol(), outputCol=\"loc_features\", vocabSize=100, minTF= 2, minDF=1)\n",
    "\n",
    "pipeline = Pipeline(stages=[term_tokenizer,term_remover,term_cv,hashtag_tokenizer,hashtag_cv,mention_tokenizer, \\\n",
    "                            mention_cv,user_tokenizer, user_cv, loc_tokenizer, loc_cv])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark operation takes - 0:11:32 which is 692 seconds in total\n"
     ]
    }
   ],
   "source": [
    "loading = time.time()\n",
    "\n",
    "model = pipeline.fit(df)\n",
    "Train_X = model.transform(df)\n",
    "\n",
    "getTime(loading)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feat = Train_X.select(\"tweet_id\",\"term_features\",\"hashtag_features\",\"mention_features\",\"user_features\",\"loc_features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(inputCols = [\"term_features\",\"hashtag_features\",\"mention_features\",\"user_features\",\"loc_features\"], outputCol=\"features\")\n",
    "transformed = assembler.transform(Train_X).select(\"tweet_id\",\"features\",\"HashTag_Birthday\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporal Split\n",
    "##### To ensure our classifier generalize to a wide range of features and not simply remeber the past hashtag, we will perform a teppral split to exclude training hashtags in validation and test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![caption](Capture.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "term_tokenizer = Tokenizer(inputCol=\"hashtag\", outputCol=\"each_hashtag\")\n",
    "hashtags_df = term_tokenizer.transform(df)\n",
    "\n",
    "hashtag =  hashtags_df.select(\"tweet_id\",\"HashTag_Birthday\",\"each_hashtag\")\n",
    "hash_exploded = hashtag.withColumn('each_hashtag', explode('each_hashtag'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Define topical hashtag list\n",
    "title = sqlContext.createDataFrame(\\\n",
    "[(\"soccer\",1,[\"princessandgino\",\"migikahitnaatkahitpa\",\"royalmigiending\",\"royalmigiendgame\",\"thankyoumikayandgino\",\"asahi\",\"jipped\",\"news\",\"litbus_anime\",\"ff\",\"mink\",\"lol\",\"happybdayharrystyles\",\"gameinsight\",\"androidgames\",\"teamheat\",\"teamnosleep\",\"android\",\"supportlocalband\"])],[\"topics\",\"topical\",\"hashtags\"]\\\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Join hashtag DF with the original DF to obtain all topical tweets for a particular topic\n",
    "\n",
    "title_exploded = title.withColumn('hashtags', explode('hashtags'))\n",
    "\n",
    "Hashtag_set = hash_exploded.join(title_exploded,\\\n",
    "                                 hash_exploded.each_hashtag == title_exploded.hashtags,\\\n",
    "                                 \"right\").select(hash_exploded.tweet_id,\\\n",
    "                                                 hash_exploded.HashTag_Birthday,\\\n",
    "                                                 hash_exploded.each_hashtag)\n",
    "## Right join to obtain all topical tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "## Find out the \"birthday\", or the earliest appearing time of each hashtag. \n",
    "## (add an extra column of 1 to mark as topical, will be used in a join later)\n",
    "Ordered_Hashtag_set = Hashtag_set.\\\n",
    "                      groupby(\"each_hashtag\").\\\n",
    "                      agg({\"Hashtag_Birthday\": \"min\"}).\\\n",
    "                      orderBy('min(Hashtag_Birthday)', ascending=True).\\\n",
    "                      withColumn(\"topical\", lit(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark operation takes - 0:01:54 which is 114 seconds in total\n"
     ]
    }
   ],
   "source": [
    "## Find the total lenth of topical tweets.\n",
    "loading = time.time()\n",
    "time_span = Ordered_Hashtag_set.count()\n",
    "getTime(loading)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get id of the corresponding time split (50% and 60%).\n",
    "\n",
    "train_val_split_Ht = np.floor(np.multiply(time_span, 0.5)).astype(int)\n",
    "val_test_split_Ht =  np.floor(np.multiply(time_span, 0.6)).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Converting to Pandas for random row access.\n",
    "\n",
    "pd_Ordered_Hashtag_set = Ordered_Hashtag_set.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# locate the timestamp of te 50% and 60% cutoff point. Will be used later to divide D.\n",
    "\n",
    "train_val_time = pd_Ordered_Hashtag_set.iloc[train_val_split_Ht]['min(Hashtag_Birthday)']\n",
    "val_test_time = pd_Ordered_Hashtag_set.iloc[val_test_split_Ht]['min(Hashtag_Birthday)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split Hashtags into H_train, H_valid, H_test\n",
    "\n",
    "train_hashtags = Ordered_Hashtag_set.select(\"each_hashtag\", \"topical\").\\\n",
    "                                     where(col(\"min(Hashtag_Birthday)\") <= train_val_time)\n",
    "    \n",
    "valid_hashtags = Ordered_Hashtag_set.select(\"each_hashtag\", \"topical\").\\\n",
    "                                     where((col(\"min(Hashtag_Birthday)\") > train_val_time) & (col(\"min(Hashtag_Birthday)\") <= val_test_time))\n",
    "    \n",
    "test_hashtags = Ordered_Hashtag_set.select(\"each_hashtag\", \"topical\").\\\n",
    "                                     where(col(\"min(Hashtag_Birthday)\") > val_test_time )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Train_ids = train_hashtags.join(Hashtag_set,\\\n",
    "                                 train_hashtags.each_hashtag == Hashtag_set.each_hashtag,\\\n",
    "                                 \"inner\").select(Hashtag_set.tweet_id,\\\n",
    "                                                 train_hashtags.topical)\n",
    "Valid_ids = valid_hashtags.join(Hashtag_set,\\\n",
    "                                 valid_hashtags.each_hashtag == Hashtag_set.each_hashtag,\\\n",
    "                                 \"inner\").select(Hashtag_set.tweet_id,\\\n",
    "                                                 valid_hashtags.topical)\n",
    "Test_ids = test_hashtags.join(Hashtag_set,\\\n",
    "                                 test_hashtags.each_hashtag == Hashtag_set.each_hashtag,\\\n",
    "                                 \"inner\").select(Hashtag_set.tweet_id,\\\n",
    "                                                 test_hashtags.topical)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Now we have identified the ids to be used in training, validation and test set, we can proceed to join the id with our feature set to obtain the corresponding data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train-Valid-Test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Training_set = transformed.select(\"tweet_id\",\"features\").where(col(\"HashTag_Birthday\") <= train_val_time)\n",
    "\n",
    "Training_set_labled = Training_set.join(Train_ids, Training_set.tweet_id == Train_ids.tweet_id, \"left\").\\\n",
    "                           drop(\"tweet_id\").\\\n",
    "                           select(Training_set.features, F.when(Train_ids.topical == 1, 1).otherwise(0).alias(\"target\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loading = time.time()\n",
    "\n",
    "pos_sample = Training_set_labled.where(col(\"target\") == 0).count()\n",
    "\n",
    "getTime(loading)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Raw_Validation_set = transformed.select(\"tweet_id\",\"features\").where((col(\"HashTag_Birthday\") > train_val_time) & (col(\"HashTag_Birthday\") <= val_test_time))\n",
    "\n",
    "tr_hashtags_in_vals  = Raw_Validation_set.\\\n",
    "                       join(Train_ids, Raw_Validation_set.tweet_id == Train_ids.tweet_id, \"inner\").\\\n",
    "                       select(Raw_Validation_set.tweet_id)                        \n",
    "\n",
    "Validation_set_staging =  Raw_Validation_set.\\\n",
    "                          join(tr_hashtags_in_vals, Raw_Validation_set.tweet_id == tr_hashtags_in_vals.tweet_id, \"left_outer\").\\\n",
    "                          toDF(\"tweet_id\",\"features\",\"new_id\")\n",
    "#### This is a huge assssss fucking bug. irect select would remove null type.\n",
    "\n",
    "Validation_set =  Validation_set_staging.select(col(\"tweet_id\"),col(\"features\")).where(col(\"new_id\").isNull())\n",
    "\n",
    "\n",
    "Validation_set_labled = Validation_set.join(Valid_ids, Validation_set.tweet_id == Valid_ids.tweet_id, \"left\").\\\n",
    "                           drop(Validation_set.tweet_id).\\\n",
    "                           select(Validation_set.features, F.when(Valid_ids.topical == 1, 1).otherwise(0).alias(\"target\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_sample = Validation_set_labled.where(col(\"target\") == 1).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Raw_Test_set = transformed.select(\"tweet_id\",\"features\").where(col(\"HashTag_Birthday\") > val_test_time)\n",
    "\n",
    "tr_hashtags_in_test  = Raw_Test_set.\\\n",
    "                       join(Train_ids, Raw_Test_set.tweet_id == Train_ids.tweet_id, \"inner\").\\\n",
    "                       select(Raw_Test_set.tweet_id)\n",
    "\n",
    "Test_set_staging =  Raw_Validation_set.\\\n",
    "                          join(tr_hashtags_in_test, Raw_Validation_set.tweet_id == tr_hashtags_in_test.tweet_id, \"left_outer\").\\\n",
    "                          toDF(\"tweet_id\",\"features\",\"new_id\")\n",
    "\n",
    "Test_set =  Test_set_staging.select(col(\"tweet_id\"),col(\"features\")).where(col(\"new_id\").isNull())\n",
    "\n",
    "\n",
    "Test_set_labled = Test_set.join(Test_ids, Test_set.tweet_id == Test_ids.tweet_id, \"left\").\\\n",
    "                           drop(\"tweet_id\").\\\n",
    "                           select(Test_set.features, F.when(Test_ids.topical == 1, 1).otherwise(0).alias(\"target\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling data to balance label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Concatenate pos and neg training samples to form the final training set.\n",
    "\n",
    "Input = Training_set_labled.sampleBy(\"target\", fractions={0: 0.5, 1: 1}, seed=0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><span style=\"color: #800000; font-size: 24pt; font-family: georgia, palatino, serif;\">Step Three: Training Classifier</span></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#As of Spark 2.0 ml and mllib API are no longer compatible and the latter one is going towards deprecation and removal. If you still need this you'll have to convert ml.Vectors to mllib.Vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib import linalg as mllib_linalg\n",
    "from pyspark.ml import linalg as ml_linalg\n",
    "\n",
    "def as_old(v):\n",
    "    if isinstance(v, ml_linalg.SparseVector):\n",
    "        return mllib_linalg.SparseVector(v.size, v.indices, v.values)\n",
    "    if isinstance(v, ml_linalg.DenseVector):\n",
    "        return mllib_linalg.DenseVector(v.values)\n",
    "    raise ValueError(\"Unsupported type {0}\".format(type(v)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.param import Param, Params\n",
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS, LogisticRegressionModel\n",
    "\n",
    "TrainingRDD=Input.rdd.map(lambda row: LabeledPoint(row.target, as_old(row.features)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+\n",
      "|    features|target|\n",
      "+------------+------+\n",
      "|(1300,[],[])|     0|\n",
      "|(1300,[],[])|     0|\n",
      "|(1300,[],[])|     0|\n",
      "|(1300,[],[])|     0|\n",
      "+------------+------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Validation_set_labled.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Valid_RDD = Validation_set_labled.rdd.map(lambda row: LabeledPoint(row.target, as_old(row.features)))\n",
    "Test_RDD = Test_set_labled.rdd.map(lambda row: LabeledPoint(row.target, as_old(row.features)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledPoint(0.0, (1300,[],[])),\n",
       " LabeledPoint(0.0, (1300,[],[])),\n",
       " LabeledPoint(0.0, (1300,[],[])),\n",
       " LabeledPoint(0.0, (1300,[],[])),\n",
       " LabeledPoint(0.0, (1300,[0],[2.0]))]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Valid_RDD.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledPoint(0.0, (1300,[],[])),\n",
       " LabeledPoint(0.0, (1300,[],[])),\n",
       " LabeledPoint(0.0, (1300,[],[])),\n",
       " LabeledPoint(0.0, (1300,[270],[2.0])),\n",
       " LabeledPoint(0.0, (1300,[],[]))]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TrainingRDD.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = LogisticRegressionWithLBFGS.train(TrainingRDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.mllib.recommendation.MatrixFactorizationModel at 0x5b1d3d0>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper Parameter Tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "\n",
    "\n",
    "# We now treat the Pipeline as an Estimator, wrapping it in a CrossValidator instance.\n",
    "# This will allow us to jointly choose parameters for all Pipeline stages.\n",
    "# A CrossValidator requires an Estimator, a set of Estimator ParamMaps, and an Evaluator.\n",
    "# We use a ParamGridBuilder to construct a grid of parameters to search over.\n",
    "# With 3 values for hashingTF.numFeatures and 2 values for lr.regParam,\n",
    "# this grid will have 3 x 2 = 6 parameter settings for CrossValidator to choose from.\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [0.0001, 0.001, 0.01, 0.1, 0.15, 0.2, 0.3, ]) \\\n",
    "    .build()\n",
    "\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=BinaryClassificationEvaluator(),\n",
    "                          numFolds=2)  # use 3+ folds in practice\n",
    "\n",
    "# Run cross-validation, and choose the best set of parameters.\n",
    "cvModel = crossval.fit(training)\n",
    "\n",
    "# Prepare test documents, which are unlabeled.\n",
    "test = spark.createDataFrame([\n",
    "    (4, \"spark i j k\"),\n",
    "    (5, \"l m n\"),\n",
    "    (6, \"mapreduce spark\"),\n",
    "    (7, \"apache hadoop\")\n",
    "], [\"id\", \"text\"])\n",
    "\n",
    "# Make predictions on test documents. cvModel uses the best model found (lrModel).\n",
    "prediction = cvModel.transform(test)\n",
    "selected = prediction.select(\"id\", \"text\", \"probability\", \"prediction\")\n",
    "for row in selected.collect():\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "#from pyspark.mllib.evaluation import RegressionMetrics, RankingMetrics\n",
    "\n",
    "# Compute raw scores on the test set\n",
    "\n",
    "predictionAndLabels = Valid_RDD.map(lambda lp: (float(model.predict(lp.features)), float(lp.label)))\n",
    "\n",
    "Pred = predictionAndLabels.map(lambda x:x[0])\n",
    "Truth = predictionAndLabels.map(lambda x:x[1])\n",
    "Pred_truth = (b.take(100), c.take(100))\n",
    "predictionAndLabels = sc.parallelize([Pred_truth])\n",
    "\n",
    "\n",
    "# Instantiate metrics object\n",
    "## Ranking metrics ONLY takes tuple of list (pred, groundtruth)\n",
    "metrics = RankingMetrics(predictionAndLabels)\n",
    "print(\"Precision @ k = %s\" % metrics.precisionAt(100)) \n",
    "\n",
    "#print(\"Mean Average precision = %s\" % metrics.meanAveragePrecision)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.predictAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.0, 0.0), (0.0, 0.0), (0.0, 0.0), (0.0, 0.0), (0.0, 0.0)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictionAndLabels.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sc.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "rf = RandomForestClassifier()\n",
    "model = rf.fit(data)\n",
    "features = model.featureImportances\n",
    "selected_features = features[:200]\n",
    "print model.featureImportances"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
