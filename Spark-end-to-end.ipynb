{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "from pyspark.sql.functions import explode\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "\n",
    "\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.param import Param, Params\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer, IDF\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import os.path\n",
    "%matplotlib inline\n",
    "pd.set_option('precision',10)\n",
    "\n",
    "import json\n",
    "import time\n",
    "#import simplejson as json\n",
    "from datetime import datetime\n",
    "from operator import add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Helper function to keep track the run time of a spark ops.\n",
    "\n",
    "def getTime(start):\n",
    "    sec = time.time() - start\n",
    "    m, s = divmod(sec, 60)\n",
    "    h, m = divmod(m, 60)\n",
    "    print('Spark operation takes - %d:%02d:%02d which is %d seconds in total' % (h,m,s,sec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark operation takes - 0:00:01 which is 1 seconds in total\n"
     ]
    }
   ],
   "source": [
    "loading = time.time()\n",
    "# full\n",
    "#data_raw = sc.textFile('/mnt/66e695cd-1a0c-4e3b-9a50-55e01b788529/Tweet_Output/Sample')\n",
    "data_raw = sc.textFile('/mnt/66e695cd-1a0c-4e3b-9a50-55e01b788529/Tweet_Output/small_sample')\n",
    "\n",
    "data = data_raw.map(lambda line: json.loads(line))  ## change encoding next time!\n",
    "sample = data.take(1)\n",
    "\n",
    "getTime(loading)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{u'HashTag_Birthday': 1359737884.0,\n",
       "  u'from_id': 87151732,\n",
       "  u'from_user': u'ishiPTI',\n",
       "  u'hashtag': u'',\n",
       "  u'location': u'loc_dha_lahore_cantt_',\n",
       "  u'mention': u'BushraShekhani',\n",
       "  u'term': u'bushrashekhani yeh ab apke students hi sach bta skte hain p',\n",
       "  u'tweet_id': 297312861586325504}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Turning to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark operation takes - 0:00:03 which is 3 seconds in total\n"
     ]
    }
   ],
   "source": [
    "## Define Dataframe schema.\n",
    "\n",
    "loading = time.time()\n",
    "schema = StructType([StructField('HashTag_Birthday', DoubleType(), False),\n",
    "                     StructField('from_id', IntegerType(), False),\n",
    "                     StructField('from_user', StringType(), False),\n",
    "                     StructField('hashtag', StringType(), True),\n",
    "                     StructField('location', StringType(), True),\n",
    "                     StructField('mention', StringType(), True),\n",
    "                     StructField('term', StringType(), True),\n",
    "                     StructField('tweet_id', StringType(), True)                     \n",
    "                    ])\n",
    "df = sqlContext.createDataFrame(data, schema)\n",
    "getTime(loading)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---------+----------+-------+--------------------+--------------+--------------------+------------------+\n",
      "|HashTag_Birthday|  from_id| from_user|hashtag|            location|       mention|                term|          tweet_id|\n",
      "+----------------+---------+----------+-------+--------------------+--------------+--------------------+------------------+\n",
      "|   1.359737884E9| 87151732|   ishiPTI|       |loc_dha_lahore_ca...|BushraShekhani|bushrashekhani ye...|297312861586325504|\n",
      "|   1.359737885E9|945063858|F5Everyday|       |  loc_brandon_ms_usa|       uziha06|rt uziha06 a manâ€™...|297312865747083264|\n",
      "|   1.359737885E9|231813876|     k30ji|       |            loc_kmkn|     belongfr7|    belongfr7 belong|297312865747103745|\n",
      "|   1.359737885E9|404571997|0infelnity|       |                loc_|  konnichimaru|        konnichimaru|297312865751281664|\n",
      "|   1.359737885E9|377993505|   nftnhni|       |      loc_singapore_|      asmooday|rt asmooday you w...|297312865772240896|\n",
      "+----------------+---------+----------+-------+--------------------+--------------+--------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temperal Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "from pyspark.sql.functions import explode\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "\n",
    "term_tokenizer = Tokenizer(inputCol=\"hashtag\", outputCol=\"each_hashtag\")\n",
    "hashtags_df = term_tokenizer.transform(df)\n",
    "\n",
    "hashtag =  hashtags_df.select(\"tweet_id\",\"HashTag_Birthday\",\"each_hashtag\")\n",
    "hash_exploded = hashtag.withColumn('each_hashtag', explode('each_hashtag'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Define topical hashtag list\n",
    "title = sqlContext.createDataFrame(\\\n",
    "[(\"soccer\",1,[\"princessandgino\",\"migikahitnaatkahitpa\",\"royalmigiending\",\"royalmigiendgame\",\"thankyoumikayandgino\",\"asahi\",\"jipped\",\"news\",\"litbus_anime\",\"ff\",\"mink\",\"lol\",\"happybdayharrystyles\",\"gameinsight\",\"androidgames\",\"teamheat\",\"teamnosleep\",\"android\",\"supportlocalband\"])],[\"topics\",\"topical\",\"hashtags\"]\\\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Join hashtag DF with the original DF to obtain all topical tweets for a particular topic\n",
    "\n",
    "title_exploded = title.withColumn('hashtags', explode('hashtags'))\n",
    "\n",
    "Hashtag_set = hash_exploded.join(title_exploded,\\\n",
    "                                 hash_exploded.each_hashtag == title_exploded.hashtags,\\\n",
    "                                 \"right\").select(hash_exploded.tweet_id,\\\n",
    "                                                 hash_exploded.HashTag_Birthday,\\\n",
    "                                                 hash_exploded.each_hashtag)\n",
    "## Right join to obtain all topical tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+----------------+------------+\n",
      "|          tweet_id|HashTag_Birthday|each_hashtag|\n",
      "+------------------+----------------+------------+\n",
      "|297312958034350080|   1.359737907E9|     android|\n",
      "|297312995808276480|   1.359737916E9|     android|\n",
      "|297313004188467200|   1.359737918E9|     android|\n",
      "+------------------+----------------+------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "Spark operation takes - 0:01:54 which is 114 seconds in total\n"
     ]
    }
   ],
   "source": [
    "loading = time.time()\n",
    "Hashtag_set.show(3)\n",
    "getTime(loading)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "## Find out the \"birthday\", or the earliest appearing time of each hashtag. \n",
    "## (add an extra column of 1 to mark as topical, will be used in a join later)\n",
    "Ordered_Hashtag_set = Hashtag_set.\\\n",
    "                      groupby(\"each_hashtag\").\\\n",
    "                      agg({\"Hashtag_Birthday\": \"min\"}).\\\n",
    "                      orderBy('min(Hashtag_Birthday)', ascending=True).\\\n",
    "                      withColumn(\"topical\", lit(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#forget about monotonic id, stick with pandas for small dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark operation takes - 0:01:56 which is 116 seconds in total\n"
     ]
    }
   ],
   "source": [
    "## Find the total lenth of topical tweets.\n",
    "loading = time.time()\n",
    "time_span = Ordered_Hashtag_set.count()\n",
    "getTime(loading)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get id of the corresponding time split (50% and 60%).\n",
    "\n",
    "train_val_split_Ht = np.floor(np.multiply(time_span, 0.5)).astype(int)\n",
    "val_test_split_Ht =  np.floor(np.multiply(time_span, 0.6)).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Converting to Pandas for random row access.\n",
    "\n",
    "pd_Ordered_Hashtag_set = Ordered_Hashtag_set.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# locate the timestamp of te 50% and 60% cutoff point. Will be used later to divide D.\n",
    "\n",
    "train_val_time = pd_Ordered_Hashtag_set.iloc[train_val_split_Ht]['min(Hashtag_Birthday)']\n",
    "val_test_time = pd_Ordered_Hashtag_set.iloc[val_test_split_Ht]['min(Hashtag_Birthday)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split Hashtags into H_train, H_valid, H_test\n",
    "\n",
    "train_hashtags = Ordered_Hashtag_set.select(\"each_hashtag\", \"topical\").\\\n",
    "                                     where(col(\"min(Hashtag_Birthday)\") <= train_val_time)\n",
    "    \n",
    "valid_hashtags = Ordered_Hashtag_set.select(\"each_hashtag\", \"topical\").\\\n",
    "                                     where((col(\"min(Hashtag_Birthday)\") > train_val_time) & (col(\"min(Hashtag_Birthday)\") <= val_test_time))\n",
    "    \n",
    "test_hashtags = Ordered_Hashtag_set.select(\"each_hashtag\", \"topical\").\\\n",
    "                                     where(col(\"min(Hashtag_Birthday)\") > val_test_time )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "loading = time.time()\n",
    "### create dataframe of hashtag, inner join .\n",
    "topical = [\"gameinsight\",\"androidgames\",\"teamheat\",\"teamnosleep\",\"android\",\"supportlocalband\"]\n",
    "Target = hash_exploded.select(hash_exploded.id, F.when(hash_exploded.each_hashtag.isin(topical), 1).otherwise(0).alias(\"relavant\"))\n",
    "topics = Target.groupBy(Target.id).agg(F.sum(Target.relavant).alias(\"topic_count\")).orderBy(\"id\")\n",
    "Y = topics.select(topics.id, F.when(topics.topic_count > 0, 1).otherwise(0).alias(\"topical\"))\n",
    "Y.show(5)\n",
    "getTime(loading)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Dpreciated\n",
    "\n",
    "Training = Train.join(Y, Train.id == Y.id, \"inner\"\\\n",
    "                     ).drop(\"id\").select(\"from_user\",\"hashtag\",\"location\",\\\n",
    "                                         \"mention\",\"term\",\"topical\").where(Y.topical == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Dpreciated\n",
    "\n",
    "Training_v2 = Train.join(Y, Train.id == Y.id, \"inner\"\\\n",
    "                     ).drop(\"id\").select(\"from_user\",\"hashtag\",\"location\",\\\n",
    "                                         \"mention\",\"term\",\"topical\").where(Y.topical == 0).limit(3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "Training = Train.join(Y, Train.id == Y.id, \"inner\"\\\n",
    "                     ).drop(\"id\",\"HashTag_Birthday\",\"from_id\").select(\"from_user\",\"hashtag\",\"location\",\\\n",
    "                                         \"mention\",\"term\",\"topical\").sampleBy(\"topical\", fractions={0: 0.1, 1: 0.1}, seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "Training = Train.join(Y, Train.id == Y.id, \"inner\"\\\n",
    "                     ).drop(\"id\").drop(\"HashTag_Birthday\").drop(\"from_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loading = time.time()\n",
    "\n",
    "#Training.show(5)\n",
    "Training_v2.show(5)\n",
    "#result = Training.unionAll(Training_v2)\n",
    "#result.show(5)\n",
    "getTime(loading)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Training.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorizing user, hashtag, location, mention, term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "term_tokenizer = Tokenizer(inputCol=\"term\", outputCol=\"words\")\n",
    "term_remover = StopWordsRemover(inputCol=term_tokenizer.getOutputCol(), outputCol=\"filtered\")\n",
    "term_cv = CountVectorizer(inputCol=term_remover.getOutputCol(), outputCol=\"term_features\", vocabSize=500, minTF= 2, minDF=1)\n",
    "#term_pipeline = Pipeline(stages=[term_tokenizer, term_remover,term_cv])\n",
    "\n",
    "\n",
    "hashtag_tokenizer = Tokenizer(inputCol=\"hashtag\", outputCol=\"tags\")\n",
    "hashtag_cv = CountVectorizer(inputCol=hashtag_tokenizer.getOutputCol(), outputCol=\"hashtag_features\", vocabSize=500, minTF = 2, minDF=1)\n",
    "#hashtag_pipeline = Pipeline(stages=[hashtag_tokenizer,hashtag_cv])\n",
    "\n",
    "mention_tokenizer = Tokenizer(inputCol=\"mention\", outputCol=\"mentions\")\n",
    "mention_cv = CountVectorizer(inputCol=mention_tokenizer.getOutputCol(), outputCol=\"mention_features\", vocabSize=100, minTF= 2, minDF=1)\n",
    "#mention_pipeline = Pipeline(stages=[mention_tokenizer, mention_cv])\n",
    "\n",
    "user_tokenizer = Tokenizer(inputCol=\"from_user\", outputCol=\"users\")\n",
    "user_cv = CountVectorizer(inputCol=user_tokenizer.getOutputCol(), outputCol=\"user_features\", vocabSize=100, minTF= 2, minDF=1)\n",
    "\n",
    "loc_tokenizer = Tokenizer(inputCol=\"location\", outputCol=\"locs\")\n",
    "loc_cv = CountVectorizer(inputCol=loc_tokenizer.getOutputCol(), outputCol=\"loc_features\", vocabSize=100, minTF= 2, minDF=1)\n",
    "\n",
    "pipeline = Pipeline(stages=[term_tokenizer,term_remover,term_cv,hashtag_tokenizer,hashtag_cv,mention_tokenizer, \\\n",
    "                            mention_cv,user_tokenizer, user_cv, loc_tokenizer, loc_cv])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark operation takes - 0:11:20 which is 680 seconds in total\n"
     ]
    }
   ],
   "source": [
    "loading = time.time()\n",
    "\n",
    "model = pipeline.fit(df)\n",
    "Train_X = model.transform(df)\n",
    "\n",
    "getTime(loading)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feat = Train_X.select(\"tweet_id\",\"term_features\",\"hashtag_features\",\"mention_features\",\"user_features\",\"loc_features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------------+----------------+----------------+-------------+------------+\n",
      "|          tweet_id|term_features|hashtag_features|mention_features|user_features|loc_features|\n",
      "+------------------+-------------+----------------+----------------+-------------+------------+\n",
      "|297312861586325504|  (500,[],[])|     (500,[],[])|     (100,[],[])|  (100,[],[])| (100,[],[])|\n",
      "|297312865747083264|  (500,[],[])|     (500,[],[])|     (100,[],[])|  (100,[],[])| (100,[],[])|\n",
      "|297312865747103745|  (500,[],[])|     (500,[],[])|     (100,[],[])|  (100,[],[])| (100,[],[])|\n",
      "|297312865751281664|  (500,[],[])|     (500,[],[])|     (100,[],[])|  (100,[],[])| (100,[],[])|\n",
      "|297312865772240896|  (500,[],[])|     (500,[],[])|     (100,[],[])|  (100,[],[])| (100,[],[])|\n",
      "+------------------+-------------+----------------+----------------+-------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "feat.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(inputCols = [\"term_features\",\"hashtag_features\",\"mention_features\",\"user_features\",\"loc_features\"], outputCol=\"features\")\n",
    "transformed = assembler.transform(Train_X).select(\"tweet_id\",\"features\",\"HashTag_Birthday\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train valid Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+----------------+-------+\n",
      "|          tweet_id|HashTag_Birthday|topical|\n",
      "+------------------+----------------+-------+\n",
      "|297312958034350080|   1.359737907E9|      1|\n",
      "|297312995808276480|   1.359737916E9|      1|\n",
      "|297313004188467200|   1.359737918E9|      1|\n",
      "|297313272619732995|   1.359737982E9|      1|\n",
      "+------------------+----------------+-------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Train_ids = train_hashtags.join(Hashtag_set,\\\n",
    "                                 train_hashtags.each_hashtag == Hashtag_set.each_hashtag,\\\n",
    "                                 \"inner\").select(Hashtag_set.tweet_id,\\\n",
    "                                                 Hashtag_set.HashTag_Birthday,\\\n",
    "                                                 train_hashtags.topical)\n",
    "## Right join to obtain all topical tweets.\n",
    "Train_ids.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Training_set = transformed.select(\"tweet_id\",\"features\").where(col(\"HashTag_Birthday\") <= train_val_time)\n",
    "\n",
    "Training_set_labled = Training_set.join(Train_ids, Training_set.tweet_id == Train_ids.tweet_id, \"left\").\\\n",
    "                           drop(\"tweet_id\").\\\n",
    "                           select(Training_set.features, F.when(Train_ids.topical == 1, 1).otherwise(0).alias(\"topical\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Training_set_labled.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Depreciated!\n",
    "\n",
    "Train_ids = hash_exploded.join(train_hashtags,\\\n",
    "                                 hash_exploded.each_hashtag == train_hashtags.each_hashtag,\\\n",
    "                                 \"right\").select(hash_exploded.id,\\\n",
    "                                                 title_exploded.topical)\n",
    "\n",
    "Training = transformed.join(Train_ids, Train.tweet_id == Y.tweet_id, \"inner\"\\\n",
    "                     ).drop(\"id\").select(\"from_user\",\"hashtag\",\"location\",\\\n",
    "                                         \"mention\",\"term\",\"topical\").where(Y.topical == 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Depreciated!\n",
    "\n",
    "Training = transformed.join(Train_ids, Train.id == Y.id, \"inner\"\\\n",
    "                     ).drop(\"id\").select(\"from_user\",\"hashtag\",\"location\",\\\n",
    "                                         \"mention\",\"term\",\"topical\").where(Y.topical == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_count = df.groupby(\"hashtag\").agg({\"Hashtag_Birthday\": \"min\"}).orderBy('min(Hashtag_Birthday)', ascending=True) #orderBy('HashTag_Birthday', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Hash_birth = train_count.select(col(\"hashtag\"), col(\"min(Hashtag_Birthday)\").cast(DecimalType(11)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Hash_birth.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    ">>> df.agg({\"age\": \"max\"}).collect()\n",
    "[Row(max(age)=5)]\n",
    ">>> from pyspark.sql import functions as F\n",
    ">>> df.agg(F.min(df.age)).collect()\n",
    "[Row(min(age)=2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sorting = df.select(\"HashTag_Birthday\").distinct().orderBy('HashTag_Birthday', ascending=True).withColumn(\"id\", monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "splitpoint = sorting.select(\"HashTag_Birthday\").where(sorting.id == 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "splitpoint.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Spark operation takes - 0:02:29 which is 149 seconds in total for 1 day\n",
    "Spark operation takes - 0:10:54 which is 654 seconds in total for 1 day full\n",
    "\n",
    "Spark operation takes - 0:03:27 which is 207 seconds in total for 4 days\n",
    "Spark operation takes - 0:34:52 which is 2092 seconds in total for 28 dyas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib import linalg as mllib_linalg\n",
    "from pyspark.ml import linalg as ml_linalg\n",
    "\n",
    "def as_old(v):\n",
    "    if isinstance(v, ml_linalg.SparseVector):\n",
    "        return mllib_linalg.SparseVector(v.size, v.indices, v.values)\n",
    "    if isinstance(v, ml_linalg.DenseVector):\n",
    "        return mllib_linalg.DenseVector(v.values)\n",
    "    raise ValueError(\"Unsupported type {0}\".format(type(v)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.param import Param, Params\n",
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS, LogisticRegressionModel\n",
    "\n",
    "\n",
    "TrainingRDD=transformed.rdd.map(lambda row: LabeledPoint(row.topical, as_old(row.features)))\n",
    "trainRDD,validRDD,testRDD=TrainingRDD.randomSplit([0.7,0.2,0.1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainRDD.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = LogisticRegressionWithLBFGS.train(trainRDD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "# Compute raw scores on the test set\n",
    "predictionAndLabels = testRDD.map(lambda lp: (float(model.predict(lp.features)), lp.label))\n",
    "\n",
    "# Instantiate metrics object\n",
    "metrics = BinaryClassificationMetrics(predictionAndLabels)\n",
    "\n",
    "# Area under precision-recall curve\n",
    "print(\"Area under PR = %s\" % metrics.areaUnderPR)\n",
    "\n",
    "# Area under ROC curve\n",
    "print(\"Area under ROC = %s\" % metrics.areaUnderROC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sc.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
